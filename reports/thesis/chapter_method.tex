\chapter{Methodology}

\section{Previous work on auditory source separation}

As this thesis is primarily concerned with musical signals, it would not be
relevant to give an overview of all audio source separation techniques presented
in the literature. The following is a brief overview of those applicable to
musical signals.

\section{Multiple fundamental frequency estimation}

This technique assumes the signal considered can be described in a format akin
to the musical score --- a collection of notes each with times indexing their
beginning and end and a frequency, the fundamental, describing the perceived
pitch of the note. Multiple fundamental frequency estimation for the purposes of music
transcription dates back to the 1970s \cite[ch.~20]{havelock2008handbook}
\cite{moorer1977transcription}. This
is related to audio source separation because the resulting high-level
representation --- the estimated score --- can be used to synthesize signals
corresponding to subsets of
notes in the score, e.g., if a particular instrument is desired, its notes are
extracted and then a signal is synthesized using stored recordings of the
instrument or instrument-modeling synthesis techniques. The technique has become
quite developed, see \cite[ch.~20]{havelock2008handbook} for a review of
modern techniques.

There are some drawbacks to the technique. Many musical signals of interest such
as unpitched percussion, do not always have a perceiveable fundamental
frequency. A musical score describes notes as having distinct boundaries in time
and frequency, which is not always true when one considers
musical gestures such as portamento or \textit{dal niente}\footnote{``Out of
    nothing'': usually accompanying a crescendo and indicating that the player
start from silence and gradually increase their playing dynamic.}. Nevertheless,
the production of even a crude score from a musical signals is useful in
applications such as automated music transcription (e.g.,
\cite{ryynanen2008automatic}) or music catalogue query
\cite{mcnab1996towards}.

\section{Principal latent component analysis (PLCA) and non-negative matrix
factorizations (NMF)}

The power spectrum of a signal and consequently its spectrogram are always
non-negative valued. If a signal is considered as consisting of a sum of
original source signals, these source signals will have non-negative
spectrograms as well. The following two techniques attempt to determine these
spectrograms solely from a spectrogram of their mixture.

\subsection{PLCA}

In this formulation, the spectrogram (defined in Section~\ref{sec:timefreqrep}),
being non-negative like a probability distribution, is considered as such
\[
    c|X(t,f)| = \mathrm{p} (t,f)
\]
where c is a constant so that the distrubution integrate to 1. Explicitly,
we consider the probability that energy in the spectrogram lie in the vicinity
of time $t$ and frequency $f$. With the hope of retrieving the spectrograms of
the $P$ underlying sources, it is proposed
that the distribution is actually the distribution of $K$ random variables each
being chosen with probability $\mathrm{p}(Z=k)$. The pair of random variables
from component $k$, $T_{k}$ and $F_{k}$ are assumed independent, i.e.,
$\mathrm{p}(t,f|Z=k)=\mathrm{p}(t|Z=k)\mathrm{p}(f|Z=k)$. Each random variable, it is
hoped, describes a source ($K=P$) or a part of a source ($K>P$). In addition,
each of these underlying distributions has marginal distributions $\mathrm{p}(t
| Z=k)$ and $\mathrm{p}(f | Z=k)$. The marginal distributions and
$\mathrm{p}(Z=k)$ can be estimated using the expectation maximization
algorithm
with the following update rules for the $l$th iteration of the
algorithm \cite{smaragdis2006probabilistic}
\[
    \mathrm{p}_{l+1}(Z=k | t, f)
    =
    \frac{\mathrm{p}_{l}(Z=k)\mathrm{p}_{l}(t|Z=k)\mathrm{p}_{l}(f|Z=k)}
    {\sum_{j=0}^{K-1}{\mathrm{p}_{l}(Z=j)\mathrm{p}_{l}(t|Z=j)\mathrm{p}_{l}(f|Z=j)}}
\]
\[
    \mathrm{p}_{l+1}(t|Z=k)
    =
    \frac{\sum_{f}\mathrm{p}(t,f)\mathrm{p}_{l+1}(Z=k|t,f)}
    {\sum_{s}\sum_{f}\mathrm{p}(s,f)\mathrm{p}_{l+1}(Z=k|s,f)}
\]
\[
    \mathrm{p}_{l+1}(f|Z=k)
    =
    \frac{\sum_{t}\mathrm{p}(t,f)\mathrm{p}_{l+1}(Z=k|t,f)}
    {\sum_{t}\sum_{g}\mathrm{p}(t,g)\mathrm{p}_{l+1}(Z=k|t,g)}
\]
\[
    \mathrm{p}_{l+1}(Z=k)
    \frac{\sum_{t}\sum_{f}\mathrm{p}(t,f)\mathrm{p}_{l+1}(Z=k|t,f)}
    {\sum_{j=0}^{K-1}\sum_{t}\sum_{f}\mathrm{p}(t,f)\mathrm{p}_{l+1}(Z=j|t,f)}
\]
After convergence, the marginal distribution $\mathrm{p}_{l^{\ast}}(t|Z=k)$
gives the distribution of energy of the $k$th component over time. Similarly, 
the marginal distribution $\mathrm{p}_{l^{\ast}}(f|Z=k)$
gives the distribution of energy of the $k$th component over frequency. Once the
set of components $\{ \tilde{k} \}$ belonging to the $p$th source has been
determined, we can synthesize the spectrogram of this source as
\[
    |X_{p}(t,f)| = \frac{1}{c}
    \sum_{j \in
    \{\tilde{k}\}}\mathrm{p}_{l^{\ast}}(t,f|Z=j)\mathrm{p}_{l^{\ast}}(Z=j)
\]

\subsection{NMF}

Instead of considering $|X(t,f)|$ as a probability distribution, we
consider it at discrete frequencies $mc_{f}$ and times $nc_{t}$ with $m,n \in
\mathbb{N}$, the entry at the $m$th row and $n$th column of matrix
$V_{m,n}=|X(nc_{t},mc_{f})|$ with non-negative entries. We seek an
approximate factorization of $\boldsymbol{V} \in \mathbb{R}^{M \times N}_{+}$
into matrices $\boldsymbol{W} \in \mathbb{R}^{M \times K}_{+}$ and
$\boldsymbol{H} \in \mathbb{R}^{K \times N}_{+}$ such that
\[
    \boldsymbol{V} \approx \boldsymbol{W}\boldsymbol{H}
\]
This can be done by solving the program
\[
    \min \mathcal{D}(\boldsymbol{V},\boldsymbol{W}\boldsymbol{H})
\]
subject to
\[
    \boldsymbol{V} \geq \boldsymbol{0}
\]
\[
    \boldsymbol{W} \geq \boldsymbol{0}
\]
\[
    \boldsymbol{H} \geq \boldsymbol{0}
\]
The particular choice of function $\mathcal(D)$ that measures divergence leads
to different update equations in the iterative procedure for finding
$\boldsymbol{W}$ and $\boldsymbol{H}$.

\subsubsection{The Kullback-Leibler divergence \cite{lee2001algorithms}}

The \textit{Kullback-Leibler} divergence function for measuring the divergence
between two matrices $\boldsymbol{X}$ and $\boldsymbol{Y}$ is
\[
    \mathcal{D}_{\text{KL}}(\boldsymbol{X},\boldsymbol{Y}) =
    \sum_{m=0}^{M-1}\sum_{n=0}^{N-1} X_{m,n}\log(Y_{m,n})-X_{m,n}+Y_{m,n}
\]
can be minimzed using the following update equations for the $l$th iteration
\[
    H_{a,b}^{l+1} =
    H_{a,b}^{l} \frac{\sum_{j=0}^{M-1}W_{j,a}^{l}V_{j,b}^{l} / (W^{l}H^{l})_{j,m}}
    {\sum_{j=0}^{M-1}W_{j,a}^{l}}
    W_{a,b}^{l+1} =
    W_{a,b}^{l} \frac{\sum_{j=0}^{N-1}H_{b,j}^{l+1}V_{a,j}^{l} / (W^{l}H^{l+1})_{a,j}}
    {\sum_{j=0}^{N-1}H_{b,j}^{l+1}}
\]
It can be shown that these update equations are equivalent to those for PLCA 
\cite{shashanka2008probabilistic}.

\subsubsection{The Itakura-Saito divergence \cite{fevotte2009nonnegative}}

Another divergence popular for factorizing spectrograms is the
\textit{Itakura-Saito} divergence 
\[
    \mathcal{D}_{\text{IS}}(\boldsymbol{X},\boldsymbol{Y}) =
    \sum_{m=0}^{M-1}\sum_{n=0}^{N-1} \frac{X_{m,n}}{Y_{m,n}}
    - \log \left(\frac{X_{m,n}}{Y_{m,n}}\right) -1
\]
This divergence is scale-invariant, meaning that
$\mathcal{D}_{\text{IS}}(c\boldsymbol{X},c\boldsymbol{Y})=\mathcal{D}_{\text{IS}}(\boldsymbol{X},\boldsymbol{Y})$,
which makes it well suited for audio signals which have a large dynamic range.
Put another way, divergences involving large values in $\boldsymbol{V}$ and
$\boldsymbol{W}\boldsymbol{H}$ will be weighted similarly to divergences
involving small values, which is not the case with the Kulback-Leibler
divergence.  The Itakura-Saito divergence is minimized through the following
update equations
\[
    \boldsymbol{H}^{l+1}
    =
    \boldsymbol{H}^{l}.
    \frac{\boldsymbol{W}^{lT}((\boldsymbol{W}^{l}\boldsymbol{H}^{l})^{.-2}.\boldsymbol{V}^{l})}
    {\boldsymbol{W}^{lT}(\boldsymbol{W}^{l}\boldsymbol{H}^{l})^{.-1}}
\]
\[
    \boldsymbol{W}^{l+1}
    =
    \boldsymbol{W}^{l}.
    \frac{((\boldsymbol{W}^{l}\boldsymbol{H}^{l+1})^{.-2}.\boldsymbol{V}^{l})\boldsymbol{H}^{l+1T}}
    {(\boldsymbol{W}^{l}\boldsymbol{H}^{l+1})^{.-1}\boldsymbol{H}^{l+1T}}
\]
Once convergence has been obtained the $k$th column of matrix $W$ will contain
values representing the level of activation of the $k$th component at the
frequency corresponding to the row index and the $k$th row of $H$ the level of
activation of the $k$th component at the time corresponding to the column index.
If the set of components $\{ \tilde{k} \}$ belonging to the $p$th source has
been determined, we can synthesize the spectrogram of this source as
\[
    |X_{p}(nc_{t},mc_{f})| =
    \sum_{j \in \{\tilde{k}\}}W_{,j}H_{j,}
\]
 
\subsection{Synthesis of factorized spectrograms}

Synthesizing the original signal is less straightforward as the phase
information contained in the STFT was discarded to obtain a non-negative
spectrogram. We can simply use the
original phases of the STFT used to compute the spectrum with the new magnitude
information from $|X_{p}(t,f)|$ but the resulting signal may have artifacts due to
the phase information of unwanted sources that remains in the STFT. A technique
to lessen these artifacts using constrained Wiener filtering has been proposed in
\cite{le2013consistent}. One may also choose to invert the spectrogram without
any phase information by using an algorithm that iteratively reconstructs the
phase part of the STFT while minimizing the error
between the spectrogram of the reconstructed signal and its original power
spectrum, starting from an initial guess \cite{griffin1984signal}. Each
iteration requires transforming a the signal to its spectrogram and then back to
a time-domain signal, requiring considerable computational effort.

\section{An approach using amplitude- and frequency-modulation}

Perceptual studies have shown that sounds modulated synchronously in amplitude
or frequency are heard as one sound, whereas asynchronously modulated sounds are
heard as distinct \cite{mcadams1989segregation} \cite{marin1991segregation}.
Here we define the modulation of parameters $\theta_i$ and $\theta_j$ as being
synchronous if they are given as functions of time, $\theta_i=f_i(t)$ and
$\theta_j=f_j(t)$ and there is an affine transform $\mathscr{A}$ such that
$\mathscr{A}\{f_i\}(t) \approx A f_j(t) + B$ where $A$ and $B$ are constants
that do not vary with time (at least for the time that we observe the signal).
If we can accurately measure these parameters and they are typical of the sounds
we are trying to separate, then we can design techniques to reliably separate
these sounds from acoustic mixtures. This involves picking those parameters
classified as belonging to the same sound, discarding the rest, and
resynthesizing from these parameters. The task of audio source separation
therefore comprises the following tasks:

\begin{itemize}
    \item
        Decide on a signal model for the sound of interest, with parameters that
        can be estimated and that are similar for similar sounds.
    \item
        Estimate the signal parameters.
    \item
        Classify the parameters and group them as sets of parameters coming from
        the same source: the sound of interest.
    \item
        Choose a group of parameters and synthesize the separated signal from
        them.
\end{itemize}

In this thesis we model the signals of interest as sums of sinsuoids with
polynomial phase. This model is chosen because the derivatives of this
polynomial are readily interpreted as the amplitude- and frequency-modulation
functions of the sinusoids and techniques exist for estimating the polynomial
coefficients from realizations of these signals. The polynomial phase sinusoid
is defined in Section~\ref{sec:polynomphasemodel}.

