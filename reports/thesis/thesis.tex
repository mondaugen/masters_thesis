\documentclass[letterpaper,12pt]{report}
\usepackage{McECEThesis}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage[final]{graphicx}
\usepackage{algorithm2e}
\usepackage{hyperref}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\newcommand\ImNe[1]{\Im \left\{ #1 \right\}}
\newcommand\ReNe[1]{\Re \left\{ #1 \right\}}
\begin{document}
\author{Nicholas Esterer}
\title{Elements of Source Separation}
\maketitle
\tableofcontents

\section{Introduction}
In signal processing, a common task is the separation of a signal with known
deterministic or statistical characteristics from another. This task has been
well studied \cite{kay1993fundamentals} \cite{hayes2009statistical}
\cite{poor2013introduction} and works well for problems of digital communication
or object detection where these characteristics are well known. In digital
communication, the signals and techniques to transmit them are often optimized
by the designer to make them robust to corruption or interference. The designers
of vehicles usually design them to be predictable and reliable and so their
positions in time will reflect this. In this thesis we tacle a more difficult
problem, that of the separation of a mixture of acoustic signals. The nature of
these signals is different in that their design criteria are either
mostly unknown or fundamentally different. For example, musical instruments are
designed to have desirable acoustic properties which are generally subjective.
As an example of one of the complications, the choirs of the orchestra are sets
of instruments actually designed to blend well; to sound as one instrument.
Ironically, it is this criterion that we use to guide the source separation
techniques described in the following.

\section{Methodology}
Perceptual studies have shown that sounds modulated synchronously in amplitude
or frequency are heard as one sound, whereas asynchronously modulated sounds are
heard as distinct \cite{mcadams1989segregation} \cite{marin1991segregation}.
Here we define the modulation of parameters $\theta_i$ and $\theta_j$ as being
synchronous if they are given as functions of time, $\theta_i=f_i(t)$ and
$\theta_j=f_j(t)$ and there is an affine transform $\mathscr{A}$ such that
$\mathscr{A}\{f_i\}(t) \approx A f_j(t) + B$ where $A$ and $B$ are constants
that do not vary with time (at least for the time that we observe the signal).
If we can accurately measure these parameters and they are typical of the sounds
we are trying to separate, then we can design techniques to reliably separate
these sounds from acoustic mixtures. This involves picking those parameters
classified as belonging to the same sound, discarding the rest, and
resynthesizing from these parameters. The task of audio source separation
therefore comprises the following tasks:

\begin{itemize}
    \item
        Decide on a signal model for the sound of interest, with parameters that
        can be estimated and that are similar for similar sounds.
    \item
        Estimate the signal parameters.
    \item
        Classify the parameters and group them as sets of parameters coming from
        the same source: the sound of interest.
    \item
        Choose a group of parameters and synthesize the separated signal from
        them.
\end{itemize}

This thesis describes various approaches to the above tasks and utilises some of
them in source separation experiments.

\section{Signal Modeling}

\subsection{Time-Frequency Representations}

As most musical instruments are resonating media and excited resonating media
are well described as linear time-invariant (LTI) auto-regressive (AR)
structures, popular models of musical audio are some variation of this
description.
% Need citation

An LTI auto-regressive structure is a signal that can be desribed using the
following \textit{difference equation}:

\begin{equation}
    x(n) = \sum_{k=1}^{K} a_k x(n-k) + b_0 v(n)
\end{equation}

Here $x$ is the output of the system (what is heard or measured) and $v$ is the
input. $K$ is the order of the model. Both are general functions of time which,
in the case of properly sampled digital audio, can be considered at times $n \in
\mathbb{Z}$ without any loss of information. $a_k,b_0 \in \mathbb{C}$ and are
constants. Casually (no pun intended) you can think of the output of the system
at time $n$ as being a linear combination of past outputs, plus some of the
scaled input.

AR structures are excited in various ways: some are bowed, others struck, etc.
To characterize the above structure we excite it with a simple signal, the
\textit{delta function}

\begin{equation}
    \delta(n) = \begin{cases}
        1 & n=0\\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

This delta function input will yield its \textit{impulse
response} from which we can derive many properties of the AR structure.

As an example take the case where $K=1$ and $a_1 = r \exp(j\omega)$, 
$r\text{,}\omega \in \mathbb{R}$, $|r|<1$ (recall that $j=\sqrt{-1}$ and is
called the \textit{imaginary number}). Then the difference equation is

\begin{equation}
    x(n) = r \exp(j\omega) x(n-1) + v(n)
\end{equation}

Exciting this with the delta function we get
\begin{equation}
    \begin{array}{c}
        x(0) = 1 \\
        x(1) = r \exp(j\omega) \\
        x(n) = r^n \exp(j\omega n)
    \end{array}
\end{equation}

which is a complex exponential starting at $n=0$ and periodic in
$n_T=\frac{2\pi}{\omega}$ multiplied by the real-valued
exponential $r^n$. In other words, the output is a damped exponential. From this
it is not hard to see that if we can estimate the coefficients $a_k$, we can
then know the frequencies, amplitudes and damping factors of the sinusoids that
are output when this structure is excited by an impulse (the delta function).
This technique is presented as a motivation for the following techniques and is
not pursued here. The interested reader is referred to \cite{makhoul1975linear}
for more information.

An alternative method for determining the frequencies and amplitudes of
sinusoids in mixture is to take the inner-product of the signal with a complex
exponential of known frequency and see what you get

\begin{equation}
    X(\omega) = \sum_{n=-\infty}^{\infty} x(n) \exp(-j \omega n)
\end{equation}

The function $X(\omega)$ will be large if $x(n)$ contains a complex exponential of
frequency $\omega$ and small if it doesn't, effectively indicating which
sinusoidal functions are present in the signal. This transformation of a signal
as a function of time $n$ into one as a function of frequency $\omega$ is known
as the \textit{Discrete-time Fourier Transform} (DTFT). 

To create a variety of pitches and timbres, typically the media of musical
instruments are not static, but vary in time. That means the sets of sinusoids
describing the state of the media and its excitation also change in time. To
account for this we consider many small intervals of signal where we assume its
characteristics are roughly static. We can then piece these time-intervals
together afterwards to get a description of the signal in both time and
frequency. To do this, we multiply the signal by a window $w$ which makes the signal
0 outside of the interval of interest. We then test what sinusoids with
frequencies $\omega$ are present at different times $\tau$, giving a function of
two variables

\begin{equation}
    X(\tau,\omega) = \sum_{n=-\infty}^{\infty} x(n) w(n - \tau) \exp(-j \omega n)
\end{equation}

This transformation of a signal of time $n$ in to one of time $\tau$ and
frequency $\omega$ is known as the \textit{Discrete-time Short-time Fourier
Transform} (DTSTFT).

\subsection{Polynomial Phase Models}

The DTFT and DTSTFT are very useful because they are invertible
\cite{portnoff1976implementation} and fast algorithms exist for their 
computation by digital computer \cite{van1992computational}. If the presence of
a sinusoid is determined, e.g., by finding $\tau^{\ast}$ and $\omega^{\ast}$ such that
$X$ is maximized, that signal can be removed or altered easily.

One drawback of these transforms is they only project onto sinusoidal functions
of linear phase, i.e., functions of constant frequency. In general, musical
signals are not linear combinations of sinusoids of constant frequency
(consider, for example, vibrato). We could decide to project onto a different
family of functions and considerable effort has been devoted to finding
alternatives (see \cite{kereliuk2011sparse} for a review). In the case of
musical signals, however, we have some prior information and can make certain
assumptions about the underlying functions.

\subsubsection{Sinusoidal Representations}
\label{sec:mqfmfromphase}
Many musical acoustic signals are quasi-harmonic, meaning that they consist of a
sum of sinusoids whose frequencies are roughly integer multiples of a
fundamental frequency. For these kinds of signals, most of the energy can be
attributed to sinusoids and so, if we neglect the noisy part of the signal, the
signal can be described by a small number of sinusoids with slowly varying
amplitude and phase, plus some noise. The model is

\begin{equation}
    x(n)=\sum_{p=1}^{P} A_p(n) \exp(j \phi_p(n)) + \varepsilon
\end{equation}

where $\varepsilon \sim \mathcal{N}(0,\sigma)$ and $A_p \in \mathbb{R}$ and
$\phi_p \in \mathbb{R}$ are functions of amplitude and phase respectively. In the
following, we consider equivalent sinusoidal mixtures of complex-valued
polynomial phase exponentials
\begin{equation}
    x(n)=\sum_{p=1}^{P} \exp(\mathcal{P}_p(n)) + \varepsilon
\end{equation}
where
\begin{equation}
    \mathcal{P}_p(n) = \sum_{q=0}^{Q} c_q n^{q}
\end{equation}

and $c_q \in \mathbb{C}$.

The sum-of-sinusoids model of McAulay and Quatieri estimates these coefficients
in an indirect way \cite{mcaulay1986speech}. Given two local maxima of the
DTSTFT $X(\tau_0,\omega_0)$ and $X(\tau_1,\omega_1)$, where $H = \tau_1 -
\tau_0$ we can conjecture a cubic
polynomial phase function for the imaginary part of the phase argument

\begin{equation}
    \tilde{\phi}(n) = c_3 (n-\tau_0)^3 + c_2 (n-\tau_0)^2 + c_1 (n-\tau_0) + c_0
\end{equation}

By noting that we have 2 measurements of the phase and frequency,
$\angle\{X(\tau_0,\omega_0)\}$ and $\angle\{X(\tau_1,\omega_1)\}$, and the frequency
is the derivative of the phase, we can solve for the coefficients of the
polynomial phase function using the following linear system of equations
\begin{equation}
    \begin{pmatrix}
        0   & 0     & 0 & 1 \\
        H^3 & H^2   & H & 1 \\
        0   & 0     & 1 & 0 \\
        3 H^2 & 2 H & 1 & 0
    \end{pmatrix}
    \begin{pmatrix}
        \Im\{c_3\} \\
        \Im\{c_2\} \\
        \Im\{c_1\} \\
        \Im\{c_0\}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \angle\{X(\tau_0,\omega_0)\} \\
        \angle\{X(\tau_1,\omega_1)\} + 2 \pi M \\
        \omega_0 \\
        \omega_1        
    \end{pmatrix}
\end{equation}
and choosing $M$ so that
\begin{equation}
    \label{eq:minfmmq}
    \int_{0}^{H}(\tilde{\phi}^{\prime\prime}(t))^{2}dt
\end{equation}
is minimized.

As only two measurements of the amplitude of the sinusoid are available,
$|X(\tau_0,\omega_0)|$ and $|X(\tau_1,\omega_1)|$, the coefficients
$c_3$ and $c_2$ are purely imaginary and the real parts of $c_1$ and $c_0$ are
determined as
\begin{equation}
    \begin{pmatrix}
        0 & 1 \\
        H & 1
    \end{pmatrix}
    \begin{pmatrix}
        \Re\{c_1\} \\
        \Re\{C_0\}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \log(|X(\tau_0,\omega_0)|) \\
        \log(|X(\tau_1,\omega_1)|)
    \end{pmatrix}
\end{equation}

\subsubsection{Polynomial phase parameter estimation}
\label{sec:ddm_description}
More recently a set of techniques have been developed that use some combination
of derivatives of the analysis window $w$ or the signal $x$ to estimate the
polynomial coefficients directly \cite{hamilton2011non}. For this thesis we will
only consider a technique that does not estimate derivatives of the signal and
only requires a once-differentiable analysis window as it is relatively easy to
implement and suits our purposes.

The following is adapted from \cite{betser2009sinusoidal}. Consider the inner
product of the signal $x(n) = \exp(\mathcal{P}_p(n)) $ and a
known analysis \textit{atom} $\psi(n)$
\[
    f(n) = \left\langle x,\psi \right\rangle =
    \int_{-\infty}^{\infty}x(n)\overline{\psi}(n)dn
\]
Differentiating with respect to $n$ we obtain
\[
    \frac{df}{dn}(n) = \frac{dx}{dn}(n)\overline{\psi}(n)
    + x(n)\frac{d\overline{\psi}}{dn}(n)
    = \left( \sum_{q=1}^{Q} q c_q n^{q-1} \right) x(n)\overline{\psi}(n)
    + x(n)\frac{d\overline{\psi}}{dn}(n)
\]

If $\psi(t)$ is 0 outside of some interval $n \in [-T,T]$ then
\[
    \int_{-T}^{T} \frac{dx}{dn}(n)\overline{\psi}(n) dn
    = \sum_{q=1}^{Q} q c_q \int_{-T}^{T} n^{q-1} x(n) \overline{\psi}(n) dn
    + \left\langle x, \frac{d\overline{\psi}}{dn} \right\rangle = 0
\]
which after rearranging is
\[ 
    \sum_{q=1}^{Q} q c_q 
    \left\langle n^{q-1} x(n) , \overline{\psi}(n) dn \right\rangle
    = -\left\langle x, \frac{d\overline{\psi}}{dn} \right\rangle
\]
From this we can see that to estimate the coefficients $c_q$, $ 1 \leq q \leq Q
$ we simply need $R$ atoms with $R \geq Q$ to solve the linear system of
equations
\begin{equation}
    \label{eq:ddmsyseq}
    \sum_{q=1}^{Q} q c_q 
    \left\langle n^{q-1} x(n) , \overline{\psi_{r}}(n) dn \right\rangle
    = -\left\langle x, \frac{d\overline{\psi_{r}}}{dn} \right\rangle
\end{equation}
for $1 \leq r \leq R$. To estimate $c_0$ we write the signal we are analysing as
\[
    s(n) = \exp(c_0) \exp \left( \sum_{q=1}^{Q} c_q n^{q} \right) + \eta (n)
\]
$\eta (n)$ is the error signal, or the part of the signal that is not explained
by our model. We also define the function $\gamma (n)$, the part of the signal
whose coefficients have already been estimated
\[
    \gamma(n) = \exp \left( \sum_{q=1}^{Q} c_q n^{q} \right)
\]
Computing the inner-product $\left\langle s , \gamma \right\rangle$, we have
\[
    \left\langle s , \gamma \right\rangle
    =
    \left\langle \exp(c_0) \gamma , \gamma \right\rangle + 
        \left\langle \eta , \gamma \right\rangle
\]
The inner-product between $\eta$ and $\gamma$ is $0$, by the orthogonality
principle \cite{kay1993fundamentals}. Furthermore, because $\exp(c_0)$ does not
depend on $n$, we have
\[
    \left\langle s , \gamma \right\rangle
    =
    \exp(c_0) \left\langle \gamma , \gamma \right\rangle
\]
so we can estimate $c_0$ as
\begin{equation}
    \label{eq:ddmestc0}
    c_0 = \log \left( \left\langle s , \gamma \right\rangle \right)
        - \log \left( \left\langle \gamma , \gamma \right\rangle \right)
\end{equation}
The estimation of the coefficients of a phase polynomial
using this method is known as the \textit{Distribution Derivative Method (DDM)}.

\subsubsection{The choice of atom $\psi$ \label{sec:optblackman}}

As we are dealing with mixtures of sinusoids of small bandwidth, in addition to
the finite time support constraint, we desire atoms whose inner-product is only
significant within a finite bandwidth of interest. To construct these atoms, we
multiply the Fourier atom by the window $w$
\[
    \psi_{\tau,\omega}^{\mathcal{F}_{w}}(n) = w(n-\tau) \exp(-j\omega(n-\tau))
\]

A good overview of different windows and their properties is given in
\cite{harris1978use}. We require that the window be at least
once-differentiable and zero outside of a certain interval, therefore, somewhat
informally, we require
\[
    \lim_{n \rightarrow T} \psi(n) = \psi(T) = 0
\]
The \textit{Hann} window possesses this property
\[
    w_{h}(n) = \begin{cases}
        0.5 + 0.5 \cos \left( \frac{n}{T}\pi \right) & -T \leq n \leq T \\
        0 & \text{otherwise}
    \end{cases}
\]

The Hann window is a member of a class of windows constructed by summing scaled
harmonically related cosine functions, subject to the constraint that the
scaling coefficients sum to 1 so that the window have a value of 1 at $n=0$.
Letting $T=N/2$, where $N$ is the length of the window
\[
    w(n) = \begin{cases}
        \sum_{m=0}^{M-1}a_{m}\cos \left( \frac{2\pi}{N}mn \right) & -\frac{N}{2} \leq n
        \leq \frac{N}{2} \\
        0 & \text{otherwise}
    \end{cases}
\]
With $M=2$ and $a_0 = a_1 = 0.5$, we have the Hann window. The
\textit{Blackman-Harris} family of windows are also sum-of-cosine windows. For
these windows, optimization techniques were used to search for coefficients
giving optimum properties, such as minimum height of the highest side-lobe
(maximum out-of-band rejection) \cite{rabiner1970approach}. The 4-term window
whose coefficients $a$ are listed in table~\ref{tab:optblackman} has a maximum
side-lobe level of 92 dB, lower than the quantization noise of a 16-bit linear
pulse code modulated signal. This window has a very large main-lobe which means
two sinusoids of similar frequency will be difficult to resolve. Furthermore,
the window has a discontinuity at its boundaries, e.g.,
$w \left( \frac{N}{2} \right) \neq 0$ and is not once-differentiable. In any
case the window is valuble in that it essentially nulls any influence of signals
outside of a bandwidth of interest.

To find a window with properties similar to the 4-term Blackman-Harris window
but without a discontinuity, we solve the optimization problem
\[
        \min||a-\tilde{a}||_2 \\
\]
subject to
\[
        w_{\tilde{a}} \left( \frac{N}{2} \right)
            = w_{\tilde{a}} \left( \frac{-N}{2} \right) = 0
\]
\[
        \sum_{m}^{M-1} a_{m} = 1
\]
where
\[
    w_{\tilde{a}}(n) = \begin{cases}
        \sum_{m=0}^{M-1}\tilde{a}_{m}\cos \left( \frac{2\pi}{N}mn \right) & -\frac{N}{2} \leq n
        \leq \frac{N}{2} \\
        0 & \text{otherwise}
    \end{cases}
\]
The solution $\tilde{a}^{\ast}$ is given in Table~\ref{tab:optblackman} and
plots are given in Figure~\ref{plot:optblackman}. This window will be referred
to as the $\mathcal{C}^{1}$ 4-Term Blackman-Harris window.

\begin{table}
    \caption{\label{tab:optblackman}}
    \begin{center}
        \begin{tabular}{l c c c c }
            Window & $a_0$ & $a_1$ & $a_2$ & $a_3$ \\
            \hline
            Minimum 4-term Blackman-Harris & 0.35857 & 0.48829 & 0.14128 &
            0.01168 \\
            $\mathcal{C}^{1}$ 4-Term Blackman-Harris & 0.35874 & 0.48831 &
            0.14127 & 0.01170
        \end{tabular}
    \end{center}
\end{table}

% Plots made with continuous_blackman_1.py
\begin{figure}
    \caption{\label{plot:opt_blackman}}
    \includegraphics[width=\textwidth]{plots/min4_blackman_td.eps}
\end{figure}

\begin{figure}
    \caption{}
    \includegraphics[width=\textwidth]{plots/min4_blackman_fd.eps}
\end{figure}

\begin{figure}
    \caption{}
    \includegraphics[width=\textwidth]{plots/c1_blackman_td.eps}
\end{figure}

\begin{figure}
    \caption{}
    \includegraphics[width=\textwidth]{plots/c1_blackman_fd.eps}
\end{figure}

\subsection{\label{sec:partialtracking}Partial Tracking}

In Section~\ref{sec:mqfmfromphase} we discussed how to determine reasonable
values for the coefficients of a cubic phase polynomial by using the frequency,
phase and time difference of two local maxima in the DTSTFT. In this section we
discuss possible ways of determining which local maxima are connected. This is
referred to as \textit{peak matching} \cite{mcaulay1986speech}
or \textit{partial tracking} \cite{smith1987parshl} \cite{depalle1993tracking}.

\subsubsection{A Greedy Method}

The original method of connecting peaks in the spectrogram is from
\cite{mcaulay1986speech}. This method is simple and fast but, as we will see,
can be sensitive to spurious peaks.

Typically the DTSTFT is computed for a block of contiguous samples, called a
\textit{frame} and these frames are computed every $H$ samples, $H$ being the
\textit{hop-size}. Consider the parameters of local maxima in adjacent frames
$k$ and $k+1$ with $M$ maxima in frame $k$ and $N$ maxima in frame $k+1$. In
\cite{mcaulay1986speech}  the parameters are the instantaneous amplitude, phase
and frequency and are indexed by frequency as $\omega_0^{k}, \dotsc,
\omega_{M-1}^{k}$ and $\omega_0^{k+1}, \dotsc, \omega_{N-1}^{k+1}$, but here we
allow for an arbitrary set of parameters $\theta_0^{k}, \dotsc,
\theta_{M-1}^{k}$ and $\theta_0^{k+1}, \dotsc,
\theta_{N-1}^{k+1}$, such as the coefficients of a phase polynomial. Define a
distance function $\mathcal{D} \left( \theta_{i},\theta_{j} \right)$ that computes the
similarity between two sets of parameters. We will now consider a method that
finds $L$ pairs of parameters that are closest.

We compute the cost matrix $C$
\[
    C = \theta^{k} \otimes_{\mathcal{D}} \theta^{k+1}
\]
so that the $i$th row and $j$th column contain $C_{i,j} = \mathcal{D} \left(
\theta_{i}^{k},\theta_{j}^{k+1} \right)$.  For each $l \in \left[0 \dotsc L-1
\right]$, find the indices $i_{l}$ and $j_{l}$ corresponding to the shortest
distance, then remove the $i_{l}$th row and $j_{l}$th column from consideration
and continue until $L$ pairs have been determined or the distances exceed some
threshold $\Delta$. This is summarized in Algorithm~\ref{alg:mq_peak_match}

\begin{algorithm}[H]
    \caption{\label{alg:mq_peak_match}}
    \KwIn{the cost matrix $C$}
    \KwOut{$L$ index pairs $\Gamma_{i}$ and $\Gamma_{j}$}
    $\Gamma_{i} \leftarrow \varnothing$\;
    $\Gamma_{j} \leftarrow \varnothing$\;
    \For{$l \leftarrow 0$ to $L-1$}{
        $i_{l},j_{l}=\argmin_{i \in \left[ 0,\dotsc,M-1 \right] \setminus
        \Gamma_{i}, j \in \left[ 0,\dotsc,M-1 \right] \setminus \Gamma_{j}}
        C_{i,j}$\;
        \If{$ C_{i_{l},j_{l}} > \Delta$}{
            \KwRet{$\Gamma_{i},\Gamma_{j}$}
        }
        $\Gamma_{i} \leftarrow \Gamma{i} \cup i_{l}$\;
        $\Gamma_{j} \leftarrow \Gamma{i} \cup j_{l}$\;
    }
    \KwRet{$\Gamma_{i},\Gamma_{j}$}
\end{algorithm}

This is a greedy algorithm because on every iteration the smallest cost is
identified and its indices are removed from consideration. Perhaps choosing a
slightly higher cost in one iteration would allow smaller costs to be chosen in
successive iterations. This algorithm does not allow for that. In other terms,
the algorithm does not find a set of pairs that represent a globally minimal sum of
costs.
Another drawback of the algorithm is that it only works between two sucessive
frames. The cost function could be extended to consider $K$ sets of parameters,
constructing an $K$-dimensional tensor instead of a matrix, but assuming equal
numbers of parameter sets in all frames, the search space would grow exponentially
with $K$. Nevertheless, the method is simple to implement, computationally
negligble, and works well with a variety of signals encountered in audio
\cite{mcaulay1986speech} \cite{smith1987parshl}.

\subsubsection{An Optimal Method}

There is a way to find a set of paths over multiple frames ($K > 2$) having the
lowest total cost if
we restrict the search to exactly $L$ paths. Instead of indexing parameters by
their frame number $k$, we make $k$ part of the parameter set so that it can be
used by the distance function $\mathcal{D}$. Assume that over $K$ frames there
are $M$ total parameter sets. We define the vector $\boldsymbol{c} \in \mathbb{R}^{M^2}$
where the entry $\boldsymbol{c}_{i + Mj} = \mathcal{D} \left( \theta_{i}, \theta_{j}
\right)$. If we have a set of connections $\Gamma_{i,j}$ we can calculate the
total cost of these connections by defining the vector
\[
    \boldsymbol{x}_{i + Mj} = \begin{cases}
        1 & \text{there is a connection between }i\text{ and }j\\
        0 & \text{otherwise}
    \end{cases}
\]
and then forming the inner product
\[
    c_{\text{total}}=\boldsymbol{c}^{T}\boldsymbol{x}
\]
Note that a node cannot be connected to itself. The question is how to find
$\boldsymbol{x}^{\ast}$ so that $c_{\text{total}}$ is minimized. If no
constraints are placed on $\boldsymbol{x}$, the solution is trivial, but not
useful. How do we constrain $\boldsymbol{x}$ to give us a solution to the
partial tracking problem? Let us consider an example.

In Figure~\ref{plot:simple_graph} we have an example of a simple graph or
lattice. The numbers are indices of nodes in the graph and the possible
connections between them are indicated by lines, or \textit{edges}. We would like to find the two
shortest paths based on different criteria. In
Figure~\ref{plot:simple_graph_greedy_paths} we find the paths using an algorithm
similar to Algorithm~\ref{alg:mq_peak_match} but search instead over a tensor
of distances $C \in \mathbb{R}^{3 \times 4 \times 2}$ whose entry $C_{i,j,k}$
represents the cost of travelling on the path connecting the $i$th node in layer
0, the $j$th node in layer 1 and the $k$th node in layer 2. This is the greedy
method of searching for the best paths whose optimality criterion is to find the
set of best paths containing the absolute best path. We see in
Figure~\ref{plot:simple_graph_greedy_paths} that the absolute shortest path, $1
\rightarrow 4 \rightarrow 8$, is discovered, followed by the second shortest
path not using the nodes of the first path.

\begin{figure}
    \caption{\label{plot:simple_graph}}
    \includegraphics[width=\textwidth]{plots/small_graph_ex.eps}
\end{figure}

\begin{figure}
    \caption{\label{plot:simple_graph_greedy_paths}}
    \includegraphics[width=\textwidth]{plots/small_graph_ex_greedy_paths.eps}
\end{figure}

To find a set of paths minimizing the total cost, we instead search for total
solutions $\boldsymbol{x}$ that describe all paths in the graph. Assume for now
that we can guarantee that the entries of $\boldsymbol{x}$ will be either 0 or
1. To find a set of constraints for our search, we consider the structure of a
valid solution $\boldsymbol{x}^{\ast}$. To maintain that paths not overlap, a
valid solution's nodes are only allowed to have one edge entering \textemdash
coming from a node in a previous frame \textemdash and one edge leaving
\textemdash going to a node in a successive frame. To translate this into a
constraint, consider the node $i$ and its possible $R$ successive connecting
nodes $j_{0} \dotsc j_{R-1}$. Define the vector
\[
    \boldsymbol{a}^{s,i}_{i + Mj_{r}} \begin{cases}
        1 & \forall j_{r} \in \left[ j_{0} \dotsc j_{R-1} \right] \\
        0 & \text{otherwise}
    \end{cases}
\]
As all the entries of $\boldsymbol{x}$ are either 0 or 1, we have
\[
    0 \leq \left\langle \boldsymbol{a}^{s,i}, \boldsymbol{x} \right\rangle \leq 1
\]
so we can make this a constraint to ensure that a node has at most one path
leaving. Similarly, if we consider the node $j$ and its possible R previous connecting
nodes $i_{0} \dotsc i_{R-1}$, the vector
\[
    \boldsymbol{a}^{p,j}_{i_{r} + Mj} \begin{cases}
        1 & \forall i_{r} \in \left[ i_{0} \dotsc i_{R-1} \right] \\
        0 & \text{otherwise}
    \end{cases}
\]
constrains that node $j$ have only one path entering through the constraint
\[
    0 \leq \left\langle \boldsymbol{a}^{p,j} , \boldsymbol{x} \right\rangle \leq 1
\]
A node on a path will also have an edge entering and an edge leaving. To
translate this into a constraint, we define a vector that counts the number of
edges entering a node and subtracts then the number of edges leaving a node. The
result should always be 0. If $r$ is the index of the node considered, the vector is simply
\[
    \boldsymbol{a}^{b,r} = \boldsymbol{a}^{p,r} - \boldsymbol{a}^{s,r}
\]
and the constraint
\[
    \left\langle \boldsymbol{a}^{b,r}, \boldsymbol{x} \right\rangle = 0
\]
Finally we want to constrain that there be only $L$ paths. We do this by
noticing that if this is true, there will be $L$ edges between frames $k$ and
$k+1$. We constrain the number of paths going from edges
$\Gamma_{k}$ in frame $k$ to $\Gamma_{k+1}$ by forming the vector
\[
    \boldsymbol{a}^{c,k} = \sum_{j \in \Gamma_{k}} \boldsymbol{a}^{s,j}
\]
and asserting the constraint
\[
    \left\langle \boldsymbol{a}^{c,k} , \boldsymbol{x} \right\rangle = 1
\]
The length of $\boldsymbol{x}$ is $M^{2}$ so the total size of all the
constraints is not insignificant, but most entries in the constraint vectors will
be 0 and therefore the resulting constraint matrices very sparse, so sparse
linear algebra routines can be used in computations. Furthermore, the
$\boldsymbol{a}^{b}$ and $\boldsymbol{a}^{c}$ constraints are derived from
$\boldsymbol{a}^{p}$ and $\boldsymbol{a}^{s}$, so only the latter need to be
stored.

The complete \textit{linear program (LP)} solving the $L$ shortest paths problem is then
\begin{samepage}
\[
        \min \boldsymbol{c}^{T}\boldsymbol{x}
\]
subject to
\[
    \boldsymbol{0} \leq
    \begin{bmatrix}
        \boldsymbol{A}_{s} \\
        \boldsymbol{A}_{p}
    \end{bmatrix} \boldsymbol{x}
    \leq \boldsymbol{1}
\]
\[
    \begin{bmatrix}
        \boldsymbol{A}_{b} \\
        \boldsymbol{A}_{c}
    \end{bmatrix}
    \boldsymbol{x}
    =
    \begin{bmatrix}
        \boldsymbol{0} \\
        \boldsymbol{1}
    \end{bmatrix}
\]
\[
    \boldsymbol{0} \leq \boldsymbol{x} \leq \boldsymbol{1}
\]
\end{samepage}
where $\boldsymbol{A}_{s}$ is the matrix with $\boldsymbol{a}^{s,m}$ as its rows
for $m \in [0 \dotsc M-1]$ and $\boldsymbol{A}_{p}$ is the matrix with
$\boldsymbol{a}^{p,m}$ as its rows, etc.

\begin{figure}
    \caption{\label{plot:simple_graph_lp_paths}}
    \includegraphics[width=\textwidth]{plots/small_graph_ex_lp_paths.eps}
\end{figure}

The solution of the two best paths using the LP formulation
is shown in Figure~\ref{plot:simple_graph_lp_paths} and a comparison of the
total costs is shown in Table~\ref{tab:greedy_lp_cost_compare}

\begin{table}
    \caption{\label{tab:greedy_lp_cost_compare} Comparison of total costs}
    \begin{center}
        \begin{tabular}{c c}
            Greedy & LP \\
            \hline
            \input{plots/small_graph_ex_greedy_cost.txt} &
            \input{plots/small_graph_ex_lp_cost.txt} \\
        \end{tabular}
    \end{center}
\end{table}

The LP formulation is based on a multiple object tracking algorithm for video
\cite{jiang2007linear}. A proof that the solution $\boldsymbol{x}^{\ast}$ will
have entries equal to either $0$ or $1$ can be found in
\cite[p.~167]{parker1988discrete}. The theoretical computational complexity of
the linear program is polynomial in the number of variables, see
\cite{karmarkar1984new} for a proof and the demonstration of a fast algorithm
for finding its solution. In practice, to extract paths from the solution, we do
not test equality with $0$ or $1$ but rather test if the solution vector's
values are greather than some threshold. This may mean that suboptimal solutions
may still be close enough. The tolerance of the solutions to suboptimality
should be investigated, as if they are tolerant, fewer iterations of a
barrier-based algorithm would be required to solve the problem. More information
on linear programming and optimization in general can be found in
\cite{boyd2004convex}.

It should be noted that in the special case that only 1 shortest path is
searched an algorithm exists that requires on the order of $N^{2}T$ calculations
\cite{rabiner1989tutorial} where $N$ is the number of nodes in each frame and
$T$ is the number of frames (assuming the same number of nodes in each frame):
this algorithm is known as the Viterbi algorithm \cite{forney1973viterbi}.

\subsubsection{Partial paths on example signal\label{sec:mq_lp_compare_chirp}}
% Add plots of crossing partials

\subsection{Classification}

If data-points consist of more than two dimensions (say $p$), it becomes
burdensome to try and find the single best or two best dimensions on which to
examine for grouping. If we consider variables on each of the dimensions that
take on the data-point's corresponding values, we are interested in the
variables that capture most of the data-points's variance. In turns out we can
determine a linear transformation of our original dataset giving $p$ variables
and their $p$ variances such that the resulting variable with the highest
variance will have the maximum variance acheivable, under some constraints that
will be explained shortly. 

\subsubsection{Principal components analysis (PCA)}

The following development is based on \cite{jolliffe2002principal}. Say we have
a set $\left\{ \boldsymbol{x} \right\}$ of data-points and their covariance
matrix $\boldsymbol{S}$. A linear function of $\boldsymbol{x}$,
$f_1(\boldsymbol{x})=\boldsymbol{a}_{1}^{T}\boldsymbol{x}$ has variance
$\sigma_{\boldsymbol{a}_{1}^{T}\boldsymbol{x}}=\boldsymbol{a}_{1}^{T}\boldsymbol{S}\boldsymbol{a}_{1}$.
Therefore, we desire a vector $a$ that maximizes
$\sigma_{\boldsymbol{a}_{1}^{T}\boldsymbol{x}}$. We can find this via the program
\[
        \max \boldsymbol{a}_{1}^{T}\boldsymbol{S}\boldsymbol{a}_{1}
\]
subject to
\[
    \boldsymbol{a}_{1}^{T}\boldsymbol{S}\boldsymbol{a}_{1}=1
\]
(to obtain a bounded solution).

The resulting function of $\boldsymbol{x}$,
$f_1(\boldsymbol{x})=\boldsymbol{a}_{1}^{\ast T}\boldsymbol{x}$ is called the first
\textit{principal component}. The second principal component
$f_2(\boldsymbol{x})=\boldsymbol{a}_{2}^{\ast T}\boldsymbol{x}$ is found similary
to the first, except with the additional constraint that it be uncorrelated
(orthogonal) to the first component, i.e.,
$\boldsymbol{a}_{2}^{\ast T}\boldsymbol{a}_{1}=0$, and the third is found by
requiring orthognality with the first two principal components, etc.

The principal components (PCs) now allow us to examine for grouping more easily as the
total variance of the dataset has been captured in the first few principal component
variables. These transformed data-points can now be classified using a
classification algorithm.

Before we continue describing classification techniques we will briefly discuss
the nature of our classification problem. If accurate and consistent
measurements of data-points can be made, and a large enough sample of
data-points is available to train a model, then to predict the classification of
new points, a function $g=\hat{h} \left( \boldsymbol{x} \right)$ is postulated,
giving the classification $g$ of a data-point $\boldsymbol{x}$. There are many
ways to determine $\hat{h}$ based on its assumed form. For example, if it is
assumed that $h \left( \boldsymbol{x} \right) =
\boldsymbol{\hat{\beta}}^{T}\boldsymbol{x}$ then a common estimator of
$\boldsymbol{\hat{\beta}}$, given $\boldsymbol{X}$, a matrix of observations of
$\boldsymbol{x}$ with the observations as its rows, and the classifications
$\boldsymbol{y}$, is \cite[p.~12]{friedman2001elements}
\begin{equation}
    \label{eq:least_squares}
    \boldsymbol{\hat{\beta}} = \left( \boldsymbol{X}^{T}\boldsymbol{X}
    \right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y}
\end{equation}
Here we do not have a sample dataset because of the large number of possible
situations and the difficulty of consistently estimating underlying model
parameters (see, for example, Section~\ref{sec:mq_lp_compare_chrip}). To automate
classification we treat the dataset beforehand to increase the robustness of the
classification algorithm, and allow the algorithm to estimate some
class memberships. The classification algorithm will give some estimated $\hat{h}$
as above, but it is doubtful that we would find a situation to use it to
classify new data-points. In the case that the classification gives a continuous
variable as in Equation~\ref{eq:least_squares}, thresholding values are
required to convert $\hat{h} \left( \boldsymbol{x} \right)$ to discrete values.
Because we would have to choose these values beforehand, this sort of
classification algorithm is probably not very useful in practice. For this
reason we opt for classification algorithms that give discrete indicators as
output, purely as a function of the input dataset. The algorithm we consider is the
\textit{Gaussian mixture model (GMM)}.

\subsubsection{Gaussian Mixture Models (GMM)}

Consider the data-points $\boldsymbol{X}$ as realizations of the vector Gaussian
distributed random variable
$X$. With a large enough sample and a small enough covariance, we will observe
realizations of $X$ as a cluster with some mean (centre point) $\boldsymbol{\mu}$ and a shape
described by the covariance matrix $\boldsymbol{\Sigma}$. If we observe multiple
clusters this might imply that there are $P$ different distributions each with
mean $\boldsymbol{\mu}_p$ and covariance matrix $\boldsymbol{\Sigma}_p$ and on each
iteration one is chosen with probability $w_p$. With $N$ observations
$\boldsymbol{x}_{n}$ we can estimate, via maximum likelihood, the $P$ sets of
parameters using a form of the \textit{expectation maximization (EM)} algorithm
\cite{moon1996expectation}, \cite{dempster1977maximum}, which is an algorithm
suitable for estimating missing data from known ones. First, define
\[
    \mathrm{p} \left( \boldsymbol{x}_{n} | p \right)
    \doteq
    \ddfrac{
        \mathcal{N} \left( \boldsymbol{x}_{n}; \boldsymbol{\mu}^{k}_{p} ,
        \boldsymbol{\Sigma}^{k}_{p} \right) w^{k}_{p}
    }{
        \sum_{l=1}^{P}
        \mathcal{N} \left( \boldsymbol{x}_{n}; \boldsymbol{\mu}^{k}_{l} ,
        \boldsymbol{\Sigma}^{k}_{l} \right) w^{k}_{l}
    }
\]
the probability that $\boldsymbol{x}_n$ given distribution $p$ (see
Appendix~\ref{apdx:gaussian_dist} for the definition of $\mathcal{N} \left(
\boldsymbol{x} ; \boldsymbol{\mu}^{k} , \boldsymbol{\Sigma}^{k} \right)$). The superscript $k$
indicates the value of this parameter on iteration $k$. To update $w^{k}_p$:
\[
    w^{k+1}_p = \frac{1}{N} \sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n |
    p \right)
\]
which means intuitively that the probability of a data-point having been
generated by distribution $p$ is the average probability of observing any
$\boldsymbol{x}_n$ given $p$. To update $\boldsymbol{\mu}^{k+1}_p$:
\[
    \boldsymbol{\mu}^{k+1}_p
    =
    \ddfrac{\sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n |
    p \right) \boldsymbol{x}_n }{\sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n |
    p \right)}
\]
which is a weighted mean of all the data-points. Those less likely for a
given $p$ will weight the mean less and vice versa. A similar computation is
made for $\boldsymbol{\Sigma}^{k+1}_p$:
\[
    \boldsymbol{\Sigma}^{k+1}_p
    =
    \ddfrac{\sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n | p \right)
        \left( \boldsymbol{x}_n - \boldsymbol{\mu}^{k+1}_p \right) \left(
        \boldsymbol{x}_n - \boldsymbol{\mu}^{k+1}_p \right)^{T}
    }{
        \sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n | p \right)
    }
\]
The algorithm is halted after some number of iterations or when convergence is
reached (the parameters change little each iteration). After convergence, the
classification $p^{\ast}$ of the data-point $\boldsymbol{x}$ is simply
\[
    p^{\ast} = \argmax_{p} \mathrm{p} \left( \boldsymbol{x}_n | p \right)
\]

\subsection{Partial synthesis}

Once the parameters have been estimated, the partials have been determined, and
the partials have been grouped into sources, we can now synthesize a single
source by choosing only those partials belonging to a single source and
resynthesizing from the parameters in some fashion. A popular technique that is
straightforward to implement is the \textit{overlap-and-add} procedure
\cite{portnoff1976implementation}, \cite{moore1990elements}. We assume that in
the neighbourhood of $\tau_{r}$ the signal is approximately described by the
function $x(n) \approx f_{\tau_{r}}(n-\tau_{r})$. To synthesize an approximation of $x$
we sum windowed $f_{\tau_{r}}$ at multiple locations, windowed by a function $w$
with finite support so
the resulting signal has finite energy and the piecewise assumption is
maintained. For simplicity we assume the $\tau_{r}$ are equally spaced by $H$
samples, and $\tau_{0}=0$, so we have $\tau_{r} = rH$. The length of the window
function $w$ is $M$ samples. The approximate signal at sample $n$ is then
\[
    \tilde{x}(n) = \sum_{l=L_{-}}^{L^{+}} w(n-lH) f_{\tau_l}(n-\tau_l)
\]
where
\[
    L_{-} = \left\lfloor \frac{n-M}{H} + 1 \right\rfloor
\]
and
\[
    L_{+} = \left\lfloor \frac{n}{H} \right\rfloor
\]
This method has some drawbacks. Usually the function $f$ is an approximation
$\tilde{f}$ of the true underlying function. In the case of partial tracking,
often partials that are too short are discarded or missed. At amplitude
transients, these short partials are important for reproducing sharp attacks
that are shorter than the window length. If these partials are missing, the
resulting signal takes on a transient similar to the window shape. This could be
overcome by choosing a window with a shape similar to the overall amplitude
envelope in the attack region when resynthesizing an attack transient.

Another drawback is that no attempt is made to interpolate between the functions
estimated at $\tau_{r}$ and $\tau_{r+1}$. This is what is carried out in
Section~\ref{sec:mqfmfromphase} and results in a cubic interpolating polynomial
for phase. From Equation~\ref{eq:ddmsyseq} we know we can estimate a polynomial
of arbitrary order for phase. We will see that using this additional information
can give us an interpolating function closer to the underlying model.

\subsection{Extended phase model}

In Section~\ref{sec:mqfmfromphase} it was shown how a cubic phase function could
be postulated from two connected measurements (see
Section~\ref{sec:partialtracking}) giving a cubic phase polynomial. Here we will
demonstrate how to include the additional phase polynomial coefficients
estimated using DDM in calculating a phase function. In a later section we will
compare the synthesis quality of the classical McAulay \textendash Quatieri
approach with our new method.

\subsubsection{Cubic phase polynomial\label{sec:cubicphasepoly} }

In this section we describe how to obtain a cubic phase polynomial from
local estimations of the coefficients of a quadratic phase polynomial.

A complex sinusoid with cubic phase has the following form:
\begin{equation}
    \label{eq:cubicphasepoly}
    \phi(n) = \exp \left(c_3 n^{3} + c_2 n^{2} + c_1 n + c_0 \right)
\end{equation}
One with quadratic phase has this form:
\begin{equation}
    \label{eq:quadraticphasepoly}
    x(n) = \exp \left(a_2 n^{2} + a_1 n + a_0 \right)
\end{equation}
with $c_{i} \in \mathbb{C}$ and $a_{i} \in \mathbb{C}$.

We can estimate the coefficients of Equation~\ref{eq:quadraticphasepoly} using the DDM. From
Equation~\ref{eq:ddmsyseq}, we define the three functions
\begin{equation}
    X_{p_{1}} \left( \tau, k \right)
    =
    \sum_{m=0}^{M-1} w(m) x(m + \tau) \exp(-j 2 \pi \frac{k m}{M})
\end{equation}
\begin{equation}
    X_{p_{2}} \left( \tau, k \right)
    =
    \sum_{m=0}^{M-1} 2 m w(m) x(m + \tau) \exp(-j 2 \pi \frac{k m}{M})
\end{equation}
\begin{equation}
    X_{w^{\prime}} \left( \tau, k \right)
    =
    -j 2 \pi \frac{k}{M} X_{p_{1}} \left( \tau , k \right) + 
    \sum_{m=0}^{M-1} \frac{dw}{dn}(m) x(m + \tau) \exp(-j 2 \pi \frac{k m}{M})
\end{equation}
Where $M$ is the length of the window and $k$ is the frequency ``bin". We also
only consider $x$ at the samples $m = [0, \dotsc, M-1]$ because we can always
shift the time reference to view an arbitrary contiguous segment of signal with
these indices.

We then find $k^{\ast}$ such that $X_{p_{1}}$ is maximum. If multiple components
are present in the signal and are sufficiently separated in frequency, we can
split the signal up into frequency bands and find local maxima. A technique for
doing so is described in \cite[p.~42]{serra1989system}. To have a system of equations
with a unique solution, take the two adjacent bins $k-1$ and $k+1$ to have
enough unique atoms for Equation~\ref{eq:ddmsyseq}. These bins
should only contain energy from the component whose parameters we are interested
in measuring \textemdash this is true if the components are adequately separated
in time and frequency. We could choose only two bins to have a non-singular
system, and there are many possibilities for choosing different atoms
\cite[p.~4639]{betser2009sinusoidal}. We choose three to have improved
estimation accuracy in situations where components are adaquately separated in
frequency. Then $a_2$ and $a_1$ can be determined by solving the linear system
\[
    \begin{pmatrix}
        X_{p_{1}} \left( \tau, k^{\ast} - 1 \right) & X_{p_{2}} \left( \tau, k^{\ast} - 1 \right) \\
        X_{p_{1}} \left( \tau, k^{\ast} \right) & X_{p_{2}} \left( \tau, k^{\ast} \right) \\
        X_{p_{1}} \left( \tau, k^{\ast} + 1 \right) & X_{p_{2}} \left( \tau, k^{\ast} + 1 \right)
    \end{pmatrix}
    \begin{pmatrix}
        a_1 \\
        a_2
    \end{pmatrix}
    =
    \begin{pmatrix}
        X_{w^{\prime}} \left( \tau, k^{\ast} - 1 \right) \\
        X_{w^{\prime}} \left( \tau, k^{\ast} \right) \\
        X_{w^{\prime}} \left( \tau, k^{\ast} + 1 \right)
    \end{pmatrix}
\]
With $a_{1}$ and $a_{2}$ determined, we can use Equation~\ref{eq:ddmestc0} to
estimate $a_0$. We will write $a^{\tau}_{i}$ to refer to coefficient $i$
determined at time $\tau$. Once the $\boldsymbol{a}^{\tau}$ have been determined
at two times $\tau_{0}$ and $\tau_{1}$, with $H = \tau_{1} - \tau_{0}$ and these
times have between determined as connected (see
Section~\ref{sec:partialtracking}), we can write a system of equations
to determine an interpolating cubic phase polynomial. To avoid numerical
instabilities and for simplicity, we shift the time origin so that $\tau_{0} =
0$. This means $c_0 = \Im \left\{ a^{\tau_0}_{0} \right\}$. To reduce the size
of the system, we require that 
\[
    \frac{d \phi}{d n} \left(\frac{H}{2} \right)
    =
    \frac{1}{2}
    \left( \Im \left\{ a^{\tau_0}_{1} \right\} + \Im \left\{ a^{\tau_1}_{1}
    \right\} \right)
\]
and
\[
    \frac{d^{2} \phi}{d n^{2}} \left(\frac{H}{2}
    \right)
    =
    \frac{1}{2} \left( \Im \left\{ a^{\tau_0}_{2} \right\} + \Im \left\{
    a^{\tau_1}_{2} \right\} \right)
\]
i.e., the frequency and first-order frequency modulation in the middle of the
segment are the average of the two measured coefficients. Finally we require
\[
    \phi \left( H \right)
    =
    \Im \left\{ a^{\tau_1}_{0} \right\} - \Im \left\{ a^{\tau_0}_{0} \right\}
    + 2 \pi U^{\ast}
\]
where $U^{\ast} \in \mathbb{Z}$ is determined to minimize
Equation~\ref{eq:minfmmq}, in this case:
\begin{equation}
    \label{eq:minfmcubic}
    \tilde{U} = \argmin_{U} \int_{0}^{H} \left(
    6 c_3 t + 2 c_2 \right)^{2} dt
\end{equation}
which is then rounded to the nearest integer to give $U^{\ast}$. To summarize we have
\begin{equation}
    \begin{pmatrix}
        H^{3} & H^{2} & H \\
        \frac{3}{4}H^{2} & H & 1 \\
        3 H & 2 & 0
    \end{pmatrix}
    \begin{pmatrix}
        c_{3} \\
        c_{2} \\
        c_{1}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \Im \left\{ a^{\tau_1}_{0} \right\} - \Im \left\{ a^{\tau_0}_{0} \right\}
            + 2 \pi U^{\ast} \\
        \frac{1}{2}
        \left( \Im \left\{ a^{\tau_0}_{1} \right\} + \Im \left\{ a^{\tau_1}_{1}
        \right\} \right) \\
        \frac{1}{2} \left( \Im \left\{ a^{\tau_0}_{2} \right\} + \Im \left\{
        a^{\tau_1}_{2} \right\} \right)
    \end{pmatrix}
\end{equation}
Solving for $c_1, \dotsc, c_3$, we have
\[
    \begin{pmatrix}
        c_{3} \\
        c_{2} \\
        c_{1}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \frac{4}{H^3} \left( \Im \left\{ a^{\tau_1}_0 \right\}
            - \Im \left\{ a^{\tau_0}_0 \right\} + 2 \pi U^{\ast} \right)
        - \frac{2}{H^{2}} \left( \Im \left\{ a^{\tau_1}_1 \right\}
            + \Im \left\{ a^{\tau_0}_1 \right\} \right) \\
        \frac{-6}{H^2} \left( \Im \left\{ a^{\tau_1}_0 \right\}
            - \Im \left\{ a^{\tau_0}_0 \right\} + 2 \pi U^{\ast} \right)
        - \frac{3}{H} \left( \Im \left\{ a^{\tau_1}_1 \right\}
            + \Im \left\{ a^{\tau_0}_1 \right\} \right)
        + \frac{1}{4}  \left( \Im \left\{ a^{\tau_0}_{2} \right\} + \Im \left\{
            a^{\tau_1}_{2} \right\} \right) \\
        \frac{-H}{4}  \left( \Im \left\{ a^{\tau_0}_{2} \right\} + \Im \left\{
            a^{\tau_1}_{2} \right\} \right) 
        + \frac{3}{H} \left( \Im \left\{ a^{\tau_1}_0 \right\}
            - \Im \left\{ a^{\tau_0}_0 \right\} + 2 \pi U^{\ast} \right)
        -  \Im \left\{ a^{\tau_1}_1 \right\}
            - \Im \left\{ a^{\tau_0}_1 \right\}
    \end{pmatrix}
\]
and then $\tilde{U}$ is determined using Equation~\ref{eq:minfmcubic} to be
\[
    \tilde{U} = \frac{1}{4 \pi} \left[ H \left( \Im \left\{ a^{\tau_1}_1
            \right\} + \Im \left\{
        a^{\tau_0}_1 \right\} \right)
        -2 \left( \Im \left\{ a^{\tau_1}_0 \right\} - \Im \left\{
        a^{\tau_0}_0 \right\} \right) \right]
\]
and then rounded to obtain $U^{\ast}$.

\subsubsection{Cubic amplitude polynomial \label{sec:cubicamppoly}}

Solving for the cubic polynomial describing the local amplitude function
\begin{equation}
    \mu(n) = \exp \left(d_3 n^{3} + d_2 n^{2} + d_1 n + d_0 \right)
\end{equation}
is more straightforward analytically as it does not require solving to maximize
the smoothness of resulting polynomial. To require continuity at the end-points
of our polynomial, we require
\[
    \mu(0) = \Re \left\{ a^{\tau_0}_0 \right\}
\]
and
\[
    \mu(H) = \Re \left\{ a^{\tau_1}_0 \right\}
\]
The first constraint is satisfied simply by setting $d_0 = \Re \left\{
a^{\tau_0}_0 \right\}$ as in Section~\ref{sec:cubicphasepoly}. The second will be
accounted for in a constrained least-squares solution for the other
coefficients. The other observations are
\[
    \frac{d \mu}{dn} (0) = \Re \left\{ a^{\tau_0}_1 \right\}
\]
\[
    \frac{d \mu}{dn} (H) = \Re \left\{ a^{\tau_1}_1 \right\}
\]
\[
    \frac{d^2 \mu}{dn^2} (0) = \Re \left\{ a^{\tau_0}_2 \right\}
\]
\[
    \frac{d^2 \mu}{dn^2} (H) = \Re \left\{ a^{\tau_1}_2 \right\}
\]
The constrained least-squares problem to be solved is then
\begin{equation}
    \begin{pmatrix}
        0 & 0 & 1 \\
        3 H^2 & 2 H & 1 \\
        0 & 2 & 0 \\
        6 H & 2 & 0
    \end{pmatrix}
    \begin{pmatrix}
        d_3 \\
        d_2 \\
        d_1
    \end{pmatrix}
    =
    \begin{pmatrix}
        \Re \left\{ a^{\tau_0}_1 \right\} \\
        \Re \left\{ a^{\tau_1}_1 \right\} \\
        \Re \left\{ a^{\tau_0}_2 \right\} \\
        \Re \left\{ a^{\tau_1}_2 \right\}
    \end{pmatrix}
\end{equation}
subject to
\[
    \begin{pmatrix}
        H^3 & H^2 & H
    \end{pmatrix}
    \begin{pmatrix}
        d_3 \\
        d_2 \\
        d_1
    \end{pmatrix}
    =
    \begin{pmatrix}
        \Re \left\{ a^{\tau_1}_0 \right\}
    \end{pmatrix}
\]
This can be solved using numerical methods, in particular, using a specific
interpretation of weighted least-squares \cite[p.~266]{golub1996matrix}.

\subsubsection{Quintic phase polynomial\label{sec:quinticphasepoly}}

Solving for the coefficients of a quintic phase polynomial is done very
similarly to Section~\ref{sec:cubicphasepoly}. The quintic phase polynomial is
\begin{equation}
    \label{eq:quinticphasepoly}
    \phi(n) = \exp \left(c_5 n^{5} + c_4 n^{4} + c_3 n^{3} + c_2 n^{2} + c_1 n + c_0 \right)
\end{equation}
We have
\[
    \phi(0) = \Im \left\{ a^{\tau_0}_0 \right\}
\]
\[
    \frac{d \phi}{d n}(0) = \Im \left\{ a^{\tau_0}_1 \right\}
\]
\[
    \frac{d^{2} \phi}{d n^{2}}(0) = \Im \left\{ a^{\tau_0}_2 \right\}
\]
and solving for the remaining coefficients is done using the linear system of
equations:
\begin{equation}
    \begin{pmatrix}
        H^5 & H^4 & H^3 \\
        5 H^4 & 4 H^3 & 3 H^2 \\
        20 H^3 & 12 H^2 & 6 H
    \end{pmatrix}
    \begin{pmatrix}
        c_5 \\
        c_4 \\
        c_3
    \end{pmatrix}
    =
    \begin{pmatrix}
        -\ImNe{ a^{\tau_0}_2 } \frac{ H^{2} }{2} - \ImNe{ a^{\tau_0}_1 } H +
            \ImNe{ a^{\tau_1}_0 } - \ImNe{ a^{\tau_1}_1 } + 2 \pi U^{\ast}\\
        -\ImNe{ a^{\tau_0}_2 } H + \ImNe{ a^{\tau_1}_1 } - \ImNe{ a^{\tau_0}_1 } \\
        \ImNe{ a^{\tau_1}_2 } - \ImNe{ a^{\tau_0}_2 }
    \end{pmatrix}
\end{equation}
The smoothness maximizing $\tilde{U}$ is found as
% M=np.round((20.*H*(w_i0+w_i1)+(H**2.)*(psi_i0-psi_i1)+40.*(phi_i0-phi_i1))/(80.*np.pi))
\[
    \tilde{U} = \frac{1}{80 \pi} \left[ 20 H ( \ImNe{ a^{\tau_0}_1 } + \ImNe{ a^{\tau_1}_1 } )
        + H^2 ( \ImNe{ a^{\tau_0}_2 } - \ImNe{ a^{\tau_1}_2 } )
        + 40 ( \ImNe{ a^{\tau_0}_0 } - \ImNe{ a^{\tau_1}_0 } ) \right]
\]
and then rounded to produce $U^{\ast}$ as above.

\subsubsection{Quintic amplitude polynomial}

Solving for the quintic amplitude polynomial
\begin{equation}
    \label{eq:quinticamppoly}
    \mu(n) = \exp \left(d_5 n^{5} + d_4 n^{4} + d_3 n^{3} + d_2 n^{2} + d_1 n + d_0 \right)
\end{equation}
is as follows:
\[
    \mu(0) = \Re \left\{ a^{\tau_0}_0 \right\}
\]
\[
    \frac{d \mu}{d n}(0) = \Re \left\{ a^{\tau_0}_1 \right\}
\]
\[
    \frac{d^{2} \mu}{d n^{2}}(0) = \Re \left\{ a^{\tau_0}_2 \right\}
\]
and solving for the remaining coefficients is done using the linear system of
equations:
\begin{equation}
    \begin{pmatrix}
        H^5 & H^4 & H^3 \\
        5 H^4 & 4 H^3 & 3 H^2 \\
        20 H^3 & 12 H^2 & 6 H
    \end{pmatrix}
    \begin{pmatrix}
        d_5 \\
        d_4 \\
        d_3
    \end{pmatrix}
    =
    \begin{pmatrix}
        -\ReNe{ a^{\tau_0}_2 } \frac{ H^{2} }{2} - \ReNe{ a^{\tau_0}_1 } H +
            \ReNe{ a^{\tau_1}_0 } - \ReNe{ a^{\tau_1}_1 } \\
        -\ReNe{ a^{\tau_0}_2 } H + \ReNe{ a^{\tau_1}_1 } - \ReNe{ a^{\tau_0}_1 } \\
        \ReNe{ a^{\tau_1}_2 } - \ReNe{ a^{\tau_0}_2 }
    \end{pmatrix}
\end{equation}

The quintic interpolating phase polynomial has been proposed in a previous paper
\cite{girin2003comparing} although they do not directly estimate the frequency
slope, choosing instead to derive it using the difference in frequency between
two analysis frames.

\subsubsection{Evaluation}

We compared the quality of an analysis-synthesis system using the
original method proposed by McAulay and Quatieri, the cubic interpolation
method, and the quartic interpolation method. A frequency- and
amplitude-modulated sinusoid was synthesized and then analysed frame-by-frame
using the DDM to estimate its initial phase (amplitude), frequency (amplitude
slope), and frequency-modulation (amplitude-modulation). Afterwards, the signal
was resynthesized using the estimated parameters and compared with the original.

The synthesized signal has 3 frequency break-points and an initial phase,
therefore its phase function can be interpolated by a cubic polynomial
\[
    x_{\phi}(n) = \exp \left(b_3 n^{3} + b_2 n^{2} + b_1 n + b_0 \right)
\]
the frequency break-points are summarized in Table~\ref{tab:cubicsinphparams}. The
initial phase is $0$ radians.

\begin{table}
    \caption{\label{tab:cubicsinphparams}}
    \begin{center}
        \begin{tabular}{l|c c c}
            Time (seconds) & 0 & 0.25 & 0.5 \\
            Frequency (Hz) & 100 & 200 & 100
        \end{tabular}
    \end{center}
\end{table}

A quartic polynomial is used for the amplitude function
\[
    x_{\mu}(n) = \exp \left(u_4 n^{4} + u_3 n^{3} + u_2 n^{2} + u_1 n + u_0 \right)
\]
and its amplitude break-points are summarized in
Table~\ref{tab:quarticsinampparams}.

\begin{table}
    \caption{\label{tab:quarticsinampparams}}
    \begin{center}
        \begin{tabular}{l|c c c c}
            Time (seconds) & 0 & 0.1 & 0.3 & 0.5 \\
            Amplitude (dB) & -10 & 0 & 0 & -10
        \end{tabular}
    \end{center}
\end{table}

The signal was sampled with a sampling rate of 16000 Hz and was analysed every
256 samples with an analysis window of length 1024 samples. For the DDM method, the
$\mathcal{C}^{1}$ 4-Term Blackman-Harris was used (see
Section~\ref{sec:optblackman}).

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_cubic_original_spec.eps}
    \caption{This depicts the log spectrum. Darker regions represent greater
        values whereas lighter are smaller values.
    \label{plot:mqcubicoriginalspec}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_cubic_estimated_spec.eps}
    \caption{This depicts the log spectrum. Darker regions represent greater
        values whereas lighter are smaller values. The estimated signal is
        computed using the original method proposed by McAulay and Quatieri.
    \label{plot:mqcubicestimatedspec}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_cubic_error.eps}
    \caption{
        The error when subtracting the original signal from the estimated
        signal. The estimated signal is computed using the original method
        proposed by McAulay and Quatieri.
    \label{plot:mqcubicerror}}
\end{figure}

%\begin{figure}
%    \includegraphics[width=\textwidth]{plots/mq_mod_cubic_original_spec.eps}
%    \caption{This depicts the dB spectrum. Darker regions represent greater
%        values whereas lighter are smaller values.
%    \label{plot:mqmodcubicoriginalspec}}
%\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_cubic_estimated_spec.eps}
    \caption{This depicts the dB spectrum. Darker regions represent greater
        values whereas lighter are smaller values. The estimated signal is
        computed using the cubic interpolation method proposed here.
    \label{plot:mqmodcubicestimatedspec}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_cubic_error.eps}
    \caption{The error when subtracting the original signal from the estimated
        signal. The estimated signal is computed using the cubic interpolation
        method proposed here.
    \label{plot:mqmodcubicerror}}
\end{figure}

%\begin{figure}
%    \includegraphics[width=\textwidth]{plots/mq_mod_quintic_original_spec.eps}
%    \caption{This depicts the dB spectrum. Darker regions represent greater
%        values whereas lighter are smaller values.
%    \label{plot:mqmodcubicoriginalspec}}
%\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_quintic_estimated_spec.eps}
    \caption{This depicts the dB spectrum. Darker regions represent greater
        values whereas lighter are smaller values. The estimated signal is
        computed using the quintic interpolation method proposed here.
    \label{plot:mqmodcubicestimatedspec}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_quintic_error.eps}
    \caption{The error when subtracting the original signal from the estimated
        signal. The estimated signal is computed using the quintic interpolation
        method proposed here.
    \label{plot:mqmodcubicerror}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_err_comp_logamp_err.eps}
    \caption{This shows the error of the interpolated signals when compared
    with the original signal for the three proposed methods.
    \label{plot:mqmoderrcomplogamperr}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_err_comp_logamp_func.eps}
    \caption{This compares the original log-amplitude function with the
    interpolated log-amplitude functions. The log-amplitude functions are
    considered because these are the real part of the polynomial exponents in
    the complex sinusoid model.
    \label{plot:mqmoderrcomplogampfunc}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_err_comp_phase_err.eps}
    \caption{This compares the phase functions with the interpolated phase
    functions. The phase functions are considered because these are the
    imaginary part of the polynomial exponents in the complex sinusoidal model.
    The error exhibited by the McAulay \textendash Quatieri interpolation is
    misleading because phase values should be compared after computing the
    remaineder when dividing by $2\pi$. This means there are only brief segments
    where the phase error is significant (compare with
    Figure~\ref{plot:mqcubicerror}). The plot is shown without adjusting the
    values to show the accuracy of the proposed methods in the interpolated
    regions.
    \label{plot:mqmoderrcompphaseerr}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_err_comp_phase_func.eps}
    \caption{\label{plot:mqmoderrcompphasefunc}}
\end{figure}

\subsubsection{Conclusion}

Out of the three proposed methods it appears that the modified cubic
interpolation method works superiorily for the signal model considered. The
reduced accuracy of the higher-order quintic model can perhaps be explained by
the ``overfitting" of the log-amplitude function (see
Figure~\ref{plot:mqmoderrcomplogampfunc}). Indeed, even the proposed cubic model
shows some overfitting in this case. From Figure~\ref{plot:mqmoderrcompphaseerr}
it is clear that the DDM based methods provide superior estimation of the phase
function -- this is not the case for the log-amplitude function. Depending on
the underlying signal, perhaps better results can be obtained by postulating a
lower-order amplitude function and higher-order phase function. The possiblity
of errors arising from numerical accuracy when evaluating the quintic
polynomials has been ruled out. We evalutated these polynomials using an
implementation of Horner's method that keeps track of the error bound
\cite[p.~95]{higham2002accuracy}: the
errors are negligible, see Figure~\ref{plot:mqmodquinticpolyevalerr} for the
results.

\begin{figure}
    \includegraphics[width=\textwidth]{plots/mq_mod_quintic_poly_eval_err.eps}
    \caption{\label{plot:mqmodquinticpolyevalerr}}
\end{figure}

\section{Experiment 2: Partial grouping in one frame}

\subsection{Introduction}
To evaluate whether the grouping of partials with common AM and FM parameters is
plausible, we synthesize a set of parameters and test by corrupting the
parameters with noise and adding spurious sets of parameters (that should not
belong to any sources).
\subsection{Methodology}
We synthesize theoretical sets of parameters as described above. On each frame
of analysis data, i.e., for parameters belonging to the same time instant, we
consider each data point as a multi-dimensional random variable. With these
random variables, we compute principal components in order to produce a variable
with maximum variance. This variable is classified using a clustering algorithm
and we evalutate the results. A summary follows:
\begin{itemize}
    \item 
        Parameters are synthesized from a theoretical mixture of AM and FM
        sinusoids.  Spurious data are added to these parameters.
    \item
        Principal components analysis is carried out on the parameters happening
        at one time instance.
    \item
        A histogram is made of the 1st principal components. Values sharing a
        bin with too few other values are discarded to remove spurious data
        points.
    \item
        Initial means and standard deviations for the Gaussian mixture models
        are made by dividing the histogram into equal parts by area and choosing
        the centres of these parts.
    \item
        The EM algorithm for Gaussian mixture models is carried out to classify
        the sources.
\end{itemize}
\subsection{Evaluation}
The algorithm is run on a typical source separation problem to evaluate its
plausibility.
\subsection{Synthesis}
Our model makes available the following parameters. Time values are in seconds,
frequency values are in $Hz$ and phase values are in radians.
\begin{itemize}
    \item
        $t$ time.
    \item
        $f_{s}$ sampling frequency.
    \item
        $N$ length of signal in samples.
    \item
        $H$ duration between data point calculations in samples (i.e., the hop
        size).
    \item
        $N_{p}$ number of sources.
    \item
        $p$ which source.
    \item
        $f_{0,p}$ fundamental frequency.
    \item
        $K_{p}$ number of harmonics.
    \item
        $k_{60,p}$ harmonic number 60 dB lower than the first.
    \item 
        $B_{p}$ the inharmonicity coefficient.
    \item
        $\phi_{0,p}$ initial phase.
    \item
        $\phi_{0,f,p}$ initial FM phase.
    \item
        $t_{60,p}$ time until amplitude of partial has dropped 60 dB.
    \item
        $t_{attack,p}$ time duration of attack portion.
    \item
        $A_{f,p}$ amplitude of FM.
    \item
        $f_{f,p}$ frequency of FM.
    \item
        $s_{p}$ the signal representing the $p$th source.
\end{itemize}

To incorporate inharmonicity often observed in real string instruments where the
strings exhibit some stiffness, we define the \emph{stretched} harmonic numbers
as follows

\begin{equation}
    K_{B}(k) = k (1+Bk^{2})^{\frac{1}{2}}
\end{equation}

Each source is synthesized using the following equation:
\begin{equation}
    s_{p}(t) = \sum_{k=1}^{K_{p}} A_{p}(k,t) \cos(2\pi
    f_{0,p}t - \frac{A_{f,p}}{f_{f,p}} \cos(2\pi f_{f,p} t +
    \phi_{0,f,p}) K_{B_{p}}(k) + \phi_{0,p})
\end{equation}
where
\begin{equation}
    A_{p}(k,t) = 
    \begin{cases}
        \exp(a_{60,p} t + a_{k,60,p}k) \cos^{2}
        (\frac{\pi}{2}(\frac{t}{t_{attack,p}} - 1)) & \text{if } t \leq
        t_{attack,p},\\
        \exp(a_{60,p} t + a_{k,60,p}k) & \text{if } t > t_{attack,p},\\
        0 & \text{otherwise}.
    \end{cases}
\end{equation}
\begin{equation}
    a_{60,p} = \frac{\log_{10}(10^{-3})}{k_{60,p}}
\end{equation}
\begin{equation}
    a_{k,60,p} = \frac{\log_{10}(10^{-3})}{t_{60,p}} 
\end{equation}
The piecewise amplitude function is based on the amplitude function of the FOF.

The estimation of these parameters is a separate problem addressed by the
Derivative or Reassignment Methods (DM and RM). We use theoretical values calculated
directly from the model signals. For interpretation, and to make it possible to
simply replace the theoretical values with those obtained from an analysis, we
compute parameters that correspond to the model of these methods.

The DM and RM seek signals $s_k \in \mathbb{C}$ of the following form:
\begin{equation}
    s_{k}(n) = \exp(\log(A_{k} + \mu_{k}n + j(\phi_{k} + \omega_{k}n +
    \frac{1}{2} \psi_{k} n^{2}))) \label{rm_model}
\end{equation}
Here $n$ is the sample number. Typically when performing a short-time analysis,
the time corresponding to $n = 0$ is made to be the centre of the window,
therefore, $t$ is the time at the centre of the window and $N_{w}$, in samples,
is the length of the middle (usually non-zero) portion of the window. The
coefficents of the $k$th harmonic of the $p$th source from our synthetic model
are given by
\begin{equation}
    \mu_{k,p}(t) = \frac{a_{60,p}}{f_{s}}
\end{equation}
\begin{equation}
    A_{k,p}(t) = \frac{1}{2} \exp(a_{60,p} t + a_{k,60,p} k)
\end{equation}
for the part of the signal after the attack portion.

For the attack portion, we estimate the parameters using least-squares on a
(rectangular) windowed signal. Let
\begin{equation}
    \hat{\mathbf{s}}(t) =
    \begin{pmatrix}
        \frac{1}{2} \exp(a_{60,p}(t - \frac{N_{w}}{2f_{s}}) + a_{k,60,p} k)
        \cos^{2}(\frac{\pi}{2}(\frac{t - \frac{N_{w}}{2f_{s}})}{t_{attack,p}} -
        1)) \\
        \vdots \\
        \frac{1}{2} \exp(a_{60,p}(t + \frac{N_{w}}{2f_{s}}) + a_{k,60,p} k)
        \cos^{2}(\frac{\pi}{2}(\frac{t + \frac{N_{w}}{2f_{s}})}{t_{attack,p}} -
        1))
    \end{pmatrix}
\end{equation}
then $\log(A_{k,p})$ and $\mu_{k,p}$ are found as the least-squares solution of
\begin{equation}
    \begin{bmatrix}
        1 & \frac{-N_{w}}{2} \\
        \vdots & \vdots \\
        1 & \frac{N_{w}}{2}
    \end{bmatrix}
    \begin{pmatrix}
        \log(A_{k,p}(t)) \\
        \mu_{k,p}(t)
    \end{pmatrix}
    = \log{\hat{\mathbf{s}}(t)}
\end{equation}

for the argument parameters (those multiplied by $j$ in \eqref{rm_model})

\begin{equation}
    \omega_{k,p}(t) = \frac{2 \pi}{f_{s}} ( f_{0,p} + A_{f,p} \sin(2 \pi f_{f,p} t +
    \phi_{0,f,p})) K_{B_{p}}(k)
\end{equation}
\begin{equation}
    \psi_{k,p}(t) = (\frac{2 \pi}{f_{s}})^{2} A_{f,p} f_{f,p} ( f_{0,p} + A_{f,p}
    \cos(2 \pi f_{f,p} t + \phi_{0,f,p})) K_{B_{p}}(k)
\end{equation}
\begin{equation}
    \phi_{k}(t) = (2\pi f_{0,p}t - \frac{A_{f,p}}{f_{f,p}} \cos(2\pi f_{f,p} t +
    \phi_{0,f,p})) K_{B_{p}}(k) + \phi_{0,p}
\end{equation}

To simulate the noise that would be present in an estimation of the signal
parameters from an arbitrary signal, we create noise corrupted values by
substituting the random variables:
\begin{itemize}
    \item
        $\tilde{\omega}_{k,p}(t) \sim
        \mathcal{N}(\omega_{k,p}(t),\omega_{no})$
    \item
        $\tilde{\psi}_{k,p}(t) \sim
        \mathcal{N}(\psi_{k,p}(t),\psi_{no})$
    \item
        $\tilde{\phi}_{k,p}(t) \sim
        \mathcal{N}(\phi_{k,p}(t),\phi_{no})$
    \item
        $\tilde{\mu}_{k,p}(t) \sim
        \mathcal{N}(\mu_{k,p}(t),\mu_{no})$
    \item
        $\tilde{A}_{k,p}(t) \sim
        \mathcal{N}(A_{k,p}(t),A_{no})$
\end{itemize}
The $\theta_{no}$ (where $\theta$ is replaced by $\omega$ etc.) specifies the
variance of the particular parameter.

We also add spurious data points as a fraction $r_{s}$ of the number of true
data points. Their values are drawn from uniform distributions with boundaries
$\theta_{s,min}$ and $\theta_{s,max}$, where $\theta$ is some parameter above,
e.g., $\omega_{s,min}$ and $\omega_{s,max}$ for the $\omega$ parameter.

Data points are computed for the times $t = 0,\frac{H}{f_{s}},\frac{2
H}{f_{s}},\ldots,\frac{\left\lfloor \frac{N}{H} \right\rfloor H}{f_{s}}$.

\subsection{Computation of Principal Components}

At each time $t$ we have $L$ data points. As the source of each data point is
now unknown, we replace the $k$ and $p$ indices with index $l$. We only consider
the amplitude and frequency modulation. According to our model, the frequency
modulation is greater for harmonics of greater centre frequency. To take this
into consideration, we divide the frequency modulation estimate $\psi_{l}(t)$ by
the constant frequency estimate $\omega_{l}(t)$ (see thesis by Creager).  The amplitude modulation
$\mu_{l}(t)$ remains constant for all harmonics of the same source, only its initial value
changes according to $k_{60,p}$.  We compile the data points at one time into a
set of observations
\begin{equation}
    \mathbf{x}_{l}(t) = \begin{pmatrix}
        \frac{\psi_{l}(t)}{\omega_{l}(t)} \\
        \mu_{l}
    \end{pmatrix}
\end{equation}
\begin{equation}
    \mathbf{X}(t) = \begin{bmatrix}
        \mathbf{x}_{1}(t) \ldots \mathbf{x}_{L}(t)
    \end{bmatrix}
\end{equation}
From these $L$ observations the correlation matrix $\mathbf{S}$ is computed. We
use the correlation matrix because the values in each row of $\mathbf{x}_{l}(t)$ do
not have the same units (see Jolliffe for a discussion about this). 

Following the standard technique for producing principal components (see
Jolliffe), we obtain a matrix $\mathbf{V}(t)$ of eigenvectors sorted so that the
eigenvector corresponding to the largest eigenvalue is in the first column, etc.
The principal components $\mathbf{A}(t)$ are then computed as
\begin{equation}
    \mathbf{A}(t) = \mathbf{V}^{T}(t)\mathbf{X}(t)
\end{equation}
We have found it sufficient to use only the first principal component and
therefore only use the values in the first row of $\mathbf{A}(t)$

If we see the $\mathbf{x}_{l}(t)$ as realizations of a random variable, the
above computation of principal components has the effect of projecting
realizations of $\mathbf{x}_{l}(t)$ to points $a_{1,l}(t)$ on a 1-dimensional
subspace. It is a fundamental theory of principal components that the
transformation above maximizes the expected euclidean distance between the
points $a_{1,l}(t)$. This is desirable for the current problem because it will
always produce a variable emphasizing the parameter with the most variance,
hence if we are observing a random variable drawn from multiple distributions
with sufficiently separated means and small enough variances, this separation
will be most observable in the principal components corresponding to the
greatest eigenvalues.

\subsection{Preparing data for clustering}
The Expectation Maximization underlying the Gaussian mixture model parameter
estimation will only converge to a local maximum (see Dempster et al), therefore, for the best
results, we compute a good initial guess and remove obvious outliers before
carrying out the clustering algorithm.

The $a_{1,l}(t)$ are compiled into a histogram of $N_{b}$ bins. The minimum and
maximum bin boundaries are computed from the maximum and minimum values of
$a_{1,l}(t)$ respectively. Values in a bin with less than $\tau_{h}$ other
values are discarded. We find $N_{p}$ contiguous sections of equal area in the
new histogram omitting the discarded values.  We use the centres of these
sections as the initial mean guesses and half their width as the distance 3
standard deviations from the mean (roughly 99.7 percent of values drawn from one
distribution will lie within this interval if they indeed follow a normal
distribution). The initial guesses for the weights are simply $\frac{1}{N_{p}}$.

\subsection{Clustering}
Gaussian mixture model parameter estimation will be discussed in an appendix of
this document. After convergence we have an estimated probability
$p(a_{1,l}(t) \text{ from distribution }p)$. We choose the
distribution $p$ for each $a_{1,l}(t)$ that gives the highest probability of it
having occured. The values $\mathbf{x}_{t}(t)$ corresponding to the $a_{1,l}(t)$
have this same classification. Those sharing the same classification can be
interpreted as coming from the same source. The figure shows the results of the
above steps carried out on a mixture of two sources synthesized with the
following parameters:
\begin{table}
    \begin{center}
        \begin{tabular}{c c c }
            Parameter & Source 1 value & Source 2 value \\
            \hline
            $f_{0,p}$ & 261.63 & 277.18 \\
            $K_{p}$ & 20 & 20 \\
            $k_{60,p}$ & 20 & 20 \\
            $B_{p}$ & 0.001 & 0.001 \\
            $\phi_{0,p}$ & 0 & 0 \\
            $\phi_{0,f,p}$ & 0 & 0.8 \\
            $t_{60,p}$ & 0.5 & 0.75 \\
            $t_{attack,p}$ & 0.1 & 0.1 \\
            $A_{f,p}$ & 11.486 & 11.486 \\
            $f_{f,p}$ & 3 & 2
        \end{tabular}
    \end{center}
\end{table}
The length of the signal $N$ is 8000 samples and the anlaysis hop size $H$ is 256
samples.

\begin{figure}
    \centering
\includegraphics[width=\textwidth,height=\textheight]{/home/sandman/Documents/development/masters_thesis/experiment_1/plots/time_frequency_spurious.ps}
\end{figure}

The data points are represented as line segments to show the frequency slope.
Data points classified as coming from the same source are dark blue and cyan,
respectively.  Data points classified as spurious are in black. Note that the
classification is done on a frame by frame basis so it could be that data points
adjacent in time that seem to come from the same source are in different
colours. This is to be addressed in the sequel.
(TODO: how do we choose levels of noise to test against? My current method is to
multiply the value by a random number drawn from a normal distribution + 1. The amount
of change is nicely interpretable as a percentage deviating from the original

\section{Experiment: Separation of two sources using partial decay rate}

In this section we demonstrate how the techniques described above can be used to
perform audio source separation. We start with a recording of an acoustic guitar
playing $a_{3}$ and a xylophone playing $f_{4}^{\sharp}$. The recordings are
from \cite{opolko1987mcgill} and have been mixed down to one channel (by simply
adding the two signals together) and resampled at 16 KHz, coded simply as a
stream of 64-bit floating-point numbers. Spectrograms of the original signals are
shown in Figure~\ref{plot:acgtra3specgram} and
Figure~\ref{plot:xylofs4specgram}. The spectrograms were produced with a
Hann window, DFT size of 4096 samples and a hop size of 512 samples.

The mixture of the two signals was analysed using the DDM for finding the
coefficients of a cubic complex phase polynomial. Local maxima in each frame
were found using the technique described in \cite[p.~42]{serra1989system}. For
each of these local maxima, the polynomial coefficients were estimated. The
analysis used the $\mathcal{C}^{1}$ 4-Term Blackman-Harris window. To obtain
partials it was then necessary to connect the local maxima. As the partials of
these two sound sources are quite stable in frequency it sufficed to use the
Viterbi algorithm \cite{forney1973viterbi} to connect local maxima in sub-bands
of the spectrum. The cost function is simply the Euclidean distance between the
frequencies of two local maxima. Partial starting points are considered in
sub-bands of width 15 Hz and these sub-bands overlap by 7.5 Hz. A partial path
starts on the first local maximum in the band exceeding -100 dB and ends at the
last maximum exceeding -100 dB. The path search algorithm will also look ahead
to further frames if no maximum is present in the next frame. Because of this,
sometimes unrealistic paths are discovered that jump between spurious maxima.
These are filtered out by discarding paths whose cost-length ratio is excessive.
See Figure~\ref{plot:acgtra3xylofs4costlengththresh} to see a plot of these
values and the thresholding function. 
\begin{figure}
    \includegraphics[width=\textwidth]{plots/partial_estimation_acgtr_xylo_pcost_vs_bound.eps}
    \caption{Paths (represented as circles) are considered only if their
    path cost is smaller than the threshold function (represented in black).\label{plot:acgtra3xylofs4costlengththresh}}
\end{figure}
The spectrogram of
the mixture can be seen in Figure~\ref{plot:acgtra3xylofs4specgram} and the
partial paths in Figure~\ref{plot:acgtra3xylofs4partials}.
\begin{figure}
    \includegraphics[width=\textwidth]{plots/partial_estimation_acgtr_xylo_specgram.eps}
    \caption{\label{plot:acgtra3xylofs4specgram}}
\end{figure}
\begin{figure}
    \includegraphics[width=\textwidth]{plots/partial_estimation_acgtr_xylo_specgram_partials.eps}
    \caption{\label{plot:acgtra3xylofs4partials}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/ac_gtr_orig_spec.eps}
    \caption{\label{plot:acgtra3specgram}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/xylo_orig_spec.eps}
    \caption{\label{plot:xylofs4specgram}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/partial_classification_acgtr_xylo_partial_trajectories.pdf}
    \caption{\label{plot:parclassaxpartialtrajectories}}
\end{figure}

A line function is fit via least-squares to each partial, as shown in
Figure~\ref{plot:parclassaxpartialtrajectories}. Our goal is to classify based
on the amplitude modulation of each partial, or to an approximation, the slope of these line-functions. We found that examining the log-length of the
partials gives better results than examining the slope directly. This is perhaps
because the log-length encodes both the starting amplitude and the slope -- recall
that the partials start on the first local maxium exceeding an amplitude
threshold. To motivate the use of this metric, we plot the log-length vs. the
starting frequency of
partials from separate analyses of the guitar and xlyophone signals and overlay
the plots (Figure~\ref{plot:partialclassificationacgtrxyloseptruememberships}).
The data points have the form
\[
    \boldsymbol{a}_i = \begin{pmatrix}
        a_{i,0} \\
        a_{i,1}
    \end{pmatrix}
\]
where $a_{i,0}$ is the first PC and $a_{i,1}$ the second. The set of PC data
points will be denoted $\{\boldsymbol{a}\}$.
\begin{figure}
    \includegraphics[width=\textwidth]{plots/partial_classification_acgtr_xylo_sep_true_memberships.eps}
    \caption{\label{plot:partialclassificationacgtrxyloseptruememberships}}
\end{figure}
We see that, for the most part, the partials belonging to the two sources are
separated appropriately into two clusters. The partials from the xylophone
present in the guitar cluster belong to higher partials, whose omission in the
final rendering of the xylophone source would not be detrimental to its
perceptual quality. Similarly, partials belonging to the guitar present in the
xylophone cluster are short and most likely belong to briefly excited modes
of the guitar body. Our intention is now to use GMM on a set of unclassified
partials to yield a plausible source separation.

GMM fitting is sensitive to its initial guess of the parameters as the algorithm
only finds a local maximum of the likelihood function
\cite{kay1993fundamentals}.
% Need to add page to citation
To find an initial guess we convolve the scatter plot with Gaussian kernels,
giving a continuous function. We use the two local maxima of this function as
the initial means for the two sought classifying Gaussian distributions. The
convolution function evalutated at $\boldsymbol{\hat{a}}$ is
\[
    f(\boldsymbol{\hat{a}}) = \sum_{\boldsymbol{a}_i \in \{\boldsymbol{a}\}}
    \mathcal{N} \left( \boldsymbol{\hat{a}} ; \boldsymbol{a}_i,
    \boldsymbol{\Sigma}_{\boldsymbol{a}} \right)
\]
%(see
%Appendix~\ref{apdx:gaussian_dist} for the definition of $\mathcal{N} \left(
%    \boldsymbol{x} ; \boldsymbol{\mu}, \boldsymbol{\Sigma} \right)$)
(see
Appendix~\ref{apdx:gaussian_dist} for the definition of $\mathcal{N}$).
To make the variance proportial to the extent of each dimension,
$\boldsymbol{\Sigma}_{\boldsymbol{a}}$ is defined as
\[
    \boldsymbol{\Sigma}_{\boldsymbol{a}}
    =
    \begin{pmatrix}
        \frac{\Delta_{\boldsymbol{a}_1}}{\Delta_{\boldsymbol{a}_0
        +\Delta_{\boldsymbol{a}_1}}}
            \theta_{\boldsymbol{\Sigma_{\boldsymbol{a}}}}
        & 0 \\
        0 & \frac{\Delta_{\boldsymbol{a}_0}}{\Delta_{\boldsymbol{a}_0
    +\Delta_{\boldsymbol{a}_1}}}
            \theta_{\boldsymbol{\Sigma_{\boldsymbol{a}}}}
    \end{pmatrix}
\]
where
\[
    \Delta_{\boldsymbol{a}_0} = \max \left( \boldsymbol{a}_0 \right)
        - \min \left( \boldsymbol{a}_0 \right)
\]
\[
    \Delta_{\boldsymbol{a}_1} = \max \left( \boldsymbol{a}_1 \right)
        - \min \left( \boldsymbol{a}_1 \right)
\]
and $\theta_{\boldsymbol{\Sigma_{\boldsymbol{a}}}}$ is a parameter to control
the smoothness of the resulting function, here
$\theta_{\boldsymbol{\Sigma_{\boldsymbol{a}}}} = 1.2$. A contour plot of the
resulting function is shown in
Figure~\ref{plot:partialclassificationacgtrxylosepestimatedmemberships}.

To initialize GMM the initial means $\boldsymbol{\mu}^{0}$ are chosen to be the
points corresponding to the local maxima of the smoothed scatter plot. To
determine initial weights $\boldsymbol{w}^{0}$ we first determine the value of
the function at the two local maxima, $f(\boldsymbol{a}_{0}^{\ast})$ and
$f(\boldsymbol{a}_{1}^{\ast})$. To weight relative to these two values, we
compute
\[
    w_{i}^{0} = \frac{\Theta_{w} \left\{ f(\boldsymbol{a}_{i}^{\ast}) \right\}}{
    \sum_{p=0}^{P-1}\Theta_{w} \left\{ f(\boldsymbol{a}_{p}^{\ast} \right\}}
\]
where $\Theta_{w}$ is some kind of weighting operator to have parametric control
over the influence of each function value. Here
\[
    \Theta_{w} \left\{ f(\boldsymbol{a}_{i}^{\ast}) \right\}
    = \begin{cases}
        f(\boldsymbol{a}_{i}^{\ast}) \theta_{w} & i = 0 \\
        f(\boldsymbol{a}_{i}^{\ast}) & \text{otherwise}
    \end{cases}
\]
and $P = 2$. For this experiment the parameter set as $\theta_{w} = 1.1$ gave
the best results. The covariance matrix $\boldsymbol{\Sigma^{0}}$ is computed as
\[
    \boldsymbol{\Sigma^{0}} = \boldsymbol{S}(\{ \boldsymbol{a} \}) +
    \boldsymbol{\epsilon}
\]
where $\boldsymbol{S}$ computes the sample covariance and
$\boldsymbol{\epsilon}$ is some small constant to avoid a singular initial covariance
matrix. 100 iterations of the EM algorithm are performed to compute
classifications. Each point is assigned to its most likely cluster using the
final estimated Gaussian distributions. The final classifications for this
classification task can be seen in
Figure~\ref{plot:partialclassificationacgtrxylosepestimatedmemberships}.

\begin{figure}
    \includegraphics[width=\textwidth]{plots/partial_classification_acgtr_xylo_estimated_memberships.eps}
    \caption{\label{plot:partialclassificationacgtrxylosepestimatedmemberships}}
\end{figure}

After the classifications have been made, synthesizing the separated sources
simply involves only synthesizing the partials classified as belonging to the
same source. For the synthesis, we use the technique described in
Section~\label{sec:cubicphasepoly}. Spectrograms of the source separated signals
are shown in Figure~\ref{plot:acgtra3specgramss} and
Figure~\ref{plot:xylofs4specgramss}. 

\begin{figure}
    \includegraphics[width=\textwidth]{plots/ac_gtr_ss_spec.eps}
    \caption{\label{plot:acgtra3specgramss}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{plots/xylo_ss_spec.eps}
    \caption{\label{plot:xylofs4specgramss}}
\end{figure}

\subsubsection{Conclusion}

Although visually it does not seem to be the case, from informal listening, the
source separation is perceptually convincing.\footnote{Soundfiles can be
downloaded from \par
\url{https://drive.google.com/file/d/0B8B4c04j8tBwZDFraEZ1dFZHRFU/view?usp=sharing}}
At least one partial from the guitar can be heard in the xylophone recording --
it is difficult to separate partials that do not have sufficient spacial separation in
Figure~\ref{plot:partialclassificationacgtrxylosepestimatedmemberships}. Another drawback of the current technique is that it requires some
tuning of the parameters $\theta_{w}$ and
$\theta_{\boldsymbol{\Sigma_{\boldsymbol{a}}}}$. Furthermore, some of the
partials from both sounds were lost in the analysis. Although a shortcoming of
the analysis rather than the classification, if partials are not sufficiently
separated in time or frequency, they cannot be separated as their analysis will
yield simply one partial when there in fact many. In any case, it is important
to see that source separation can be carried out by only considering the
amplitude modulation (in this case, the decay rate) in relation to the partial
frequency -- the techniques discussed in Section~\ref{sec:previoussstechniques}
use fundamental frequency estimation or harmonicity assumptions in their models.


\bibliographystyle{plain}
\bibliography{thesis}
\end{document}
