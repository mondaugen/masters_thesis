\chapter{Gaussian Mixture Models (GMM) \label{chap:gmm}}

Consider the data-points $\boldsymbol{X}$ as realizations of the vector Gaussian
distributed random variable $X$. With a large enough sample and a small enough
covariance, we will observe realizations of $X$ as a cluster with some mean
(centre point) $\boldsymbol{\mu}$ and a shape described by the covariance matrix
$\boldsymbol{\Sigma}$. If we observe multiple clusters this might imply that
there are $P$ different distributions each with mean $\boldsymbol{\mu}_p$ and
covariance matrix $\boldsymbol{\Sigma}_p$ and on each iteration one is chosen
with probability $w_p$. With $N$ observations $\boldsymbol{x}_{n}$ we can
estimate, via maximum likelihood, the $P$ sets of parameters using a form of the
\textit{expectation maximization (EM)} algorithm \cite{moon1996expectation},
\cite{dempster1977maximum}, which is an algorithm suitable for estimating
missing data from known ones. First, define
\[
    \mathrm{p} \left( \boldsymbol{x}_{n} | p \right)
    =
    \ddfrac{
        \mathcal{N} \left( \boldsymbol{x}_{n}; \boldsymbol{\mu}^{k}_{p} ,
        \boldsymbol{\Sigma}^{k}_{p} \right) w^{k}_{p}
    }{
        \sum_{l=1}^{P}
        \mathcal{N} \left( \boldsymbol{x}_{n}; \boldsymbol{\mu}^{k}_{l} ,
        \boldsymbol{\Sigma}^{k}_{l} \right) w^{k}_{l}
    }
\]
the probability that $\boldsymbol{x}_n$ given distribution $p$ (see
Appendix~\ref{apdx:gaussian_dist} for the definition of $\mathcal{N} \left(
\boldsymbol{x} ; \boldsymbol{\mu}^{k} , \boldsymbol{\Sigma}^{k} \right)$). The superscript $k$
indicates the value of this parameter on iteration $k$. To update $w^{k}_p$:
\[
    w^{k+1}_p = \frac{1}{N} \sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n |
    p \right)
\]
which means intuitively that the probability of a data-point having been
generated by distribution $p$ is the average probability of observing any
$\boldsymbol{x}_n$ given $p$. To update $\boldsymbol{\mu}^{k+1}_p$:
\[
    \boldsymbol{\mu}^{k+1}_p
    =
    \ddfrac{\sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n |
    p \right) \boldsymbol{x}_n }{\sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n |
    p \right)}
\]
which is a weighted mean of all the data-points. Those less likely for a
given $p$ will weight the mean less and vice versa. A similar computation is
made for $\boldsymbol{\Sigma}^{k+1}_p$:
\[
    \boldsymbol{\Sigma}^{k+1}_p
    =
    \ddfrac{\sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n | p \right)
        \left( \boldsymbol{x}_n - \boldsymbol{\mu}^{k+1}_p \right) \left(
        \boldsymbol{x}_n - \boldsymbol{\mu}^{k+1}_p \right)^{T}
    }{
        \sum_{n=1}^{N} \mathrm{p} \left( \boldsymbol{x}_n | p \right)
    }
\]
The algorithm is halted after some number of iterations or when convergence is
reached, i.e., the parameters change little each iteration. After convergence, the
classification $p^{\ast}$ of the data-point $\boldsymbol{x}$ is simply
\[
    p^{\ast} = \argmax_{p} \mathrm{p} \left( \boldsymbol{x}_n | p \right)
\]

