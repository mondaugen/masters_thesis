\chapter{Principal components analysis (PCA)\label{chap:pca}}

\section{Motivation}

If data-points consist of more than two dimensions (say $p$), it becomes
burdensome to try and find the single best or two best dimensions on which to
examine for grouping. If we consider variables on each of the dimensions that
take on the data-point's corresponding values, we are interested in the
variables that capture most of the data-points's variance. In turns out we can
determine a linear transformation of our original dataset giving $p$ variables
and their $p$ variances such that the resulting variable with the highest
variance will have the maximum variance acheivable, under some constraints that
will be explained shortly. 

\section{Computation of principal components}

The following development is based on \cite{jolliffe2002principal}. Say we have
a set $\left\{ \boldsymbol{x} \right\}$ of data-points and their covariance
matrix $\boldsymbol{S}$. A linear function of $\boldsymbol{x}$,
$f_1(\boldsymbol{x})=\boldsymbol{a}_{1}^{T}\boldsymbol{x}$ has variance
$\sigma_{\boldsymbol{a}_{1}^{T}\boldsymbol{x}}=\boldsymbol{a}_{1}^{T}\boldsymbol{S}\boldsymbol{a}_{1}$.
Therefore, we desire a vector $a$ that maximizes
$\sigma_{\boldsymbol{a}_{1}^{T}\boldsymbol{x}}$. We can find this via the program
\[
        \max \boldsymbol{a}_{1}^{T}\boldsymbol{S}\boldsymbol{a}_{1}
\]
subject to
\[
    \boldsymbol{a}_{1}^{T}\boldsymbol{S}\boldsymbol{a}_{1}=1
\]
(to obtain a bounded solution).

The resulting function of $\boldsymbol{x}$,
$f_1(\boldsymbol{x})=\boldsymbol{a}_{1}^{\ast T}\boldsymbol{x}$ is called the first
\textit{principal component}. The second principal component
$f_2(\boldsymbol{x})=\boldsymbol{a}_{2}^{\ast T}\boldsymbol{x}$ is found similary
to the first, except with the additional constraint that it be uncorrelated
(orthogonal) to the first component, i.e.,
$\boldsymbol{a}_{2}^{\ast T}\boldsymbol{a}_{1}=0$, and the third is found by
requiring orthognality with the first two principal components, etc.

The principal components (PCs) now allow us to examine for grouping more easily as the
total variance of the dataset has been captured in the first few principal component
variables. These transformed data-points can now be classified using a
classification algorithm.

Before we continue describing classification techniques we will briefly discuss
the nature of our classification problem. If accurate and consistent
measurements of data-points can be made, and a large enough sample of
data-points is available to train a model, then to predict the classification of
new points, a function $g=\hat{h} \left( \boldsymbol{x} \right)$ is postulated,
giving the classification $g$ of a data-point $\boldsymbol{x}$. For example, if
there are only two classes, we might postulate a linear
classifier
\[
    \hat{h}(\boldsymbol{x}) = \begin{cases}
        0 \text{ if } \boldsymbol{\beta}^{T}\boldsymbol{x} > c \\
        1 \text{ if } \boldsymbol{\beta}^{T}\boldsymbol{x} < c
    \end{cases}
\]
where 0 and 1 indicate membership in class 0 or 1. Techniques for choosing
$\hat{h}$ are discussed in \cite{friedman2001elements} and often require data on
which to ``train'' a classifier, i.e., determine a function $\hat{h}$ that
classifies well a dataset with known classifications.  Here we do not have a
sample dataset because of the large number of possible situations and the
difficulty of consistently estimating underlying model parameters (see, for
example, Section~\ref{sec:mq_lp_compare_chirp}). As training to find a suitable
$\hat{h}$ is not possible, we propose \textit{kernels} that reflect the
hypothesized underlying structure of the distribution of $\boldsymbol{x}$. To
elaborate, we may observe that the classification of a
point is indicated by its nearness to other points in the same class, i.e., its
membership to a cluster. The proposed kernel is a parameterized model of a
cluster, whose parameters we can adjust until they fit the observed data well.
We then choose the kernel that best explains the data to indicate what class
these data are in. The following section describes a technique using normal
distributions as kernels, which is the technique used in later experiments.
